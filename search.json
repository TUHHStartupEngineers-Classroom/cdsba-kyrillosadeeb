[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is a template example for lab journaling. Students in the data science courses at the Institute of Entrepreneurship will use this template to learn R for business analytics. Students can replace this text as they wish."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website every time before you want to upload changes."
  },
  {
    "objectID": "content/01_journal/02_statistics.html",
    "href": "content/01_journal/02_statistics.html",
    "title": "Statistical Concepts",
    "section": "",
    "text": "library(dplyr)\n\nrandom_vars &lt;- readRDS(\"~/Documents/TUHH/Casual Data Science for Business Analytics/Causal_Data_Science_Data/random_vars.rds\")\n\n\n\n\nageExpectedValue &lt;- summarise(random_vars, mean = mean(age))\nprint(ageExpectedValue)\n\n#&gt; # A tibble: 1 × 1\n#&gt;    mean\n#&gt;   &lt;dbl&gt;\n#&gt; 1  33.5\n\nageVariance &lt;- var(random_vars[,1])\nprint(ageVariance)\n\n#&gt;          age\n#&gt; age 340.6078\n\nageStandardDeviation &lt;- summarise(random_vars, sd = sd(age))\nprint(ageStandardDeviation)\n\n#&gt; # A tibble: 1 × 1\n#&gt;      sd\n#&gt;   &lt;dbl&gt;\n#&gt; 1  18.5\n\n\n\n\n\n\nincomeExpectedValue &lt;- summarise(random_vars, mean = mean(income))\nprint(incomeExpectedValue)\n\n#&gt; # A tibble: 1 × 1\n#&gt;    mean\n#&gt;   &lt;dbl&gt;\n#&gt; 1 3511.\n\nincomeVariance &lt;- var(random_vars[,2])\nprint(incomeVariance)\n\n#&gt;         income\n#&gt; income 8625646\n\nincomeStandardDeviation &lt;- summarise(random_vars, sd = sd(income))\nprint(incomeStandardDeviation)\n\n#&gt; # A tibble: 1 × 1\n#&gt;      sd\n#&gt;   &lt;dbl&gt;\n#&gt; 1 2937.\n\n\n\n\n\n\nWell I believe that there’s no sense to compare the standard deviations because simple the income and age aren’t directly related and hence we cannot say that if someone’s age is above the mean with a certain deviation that their income will increase by a certain deviation and vice versa\n\n\n\n\ncov &lt;- cov(random_vars[,1],random_vars[,2])\nprint(cov)\n\n#&gt;       income\n#&gt; age 29700.15\n\ncor &lt;- cor(random_vars[,1],random_vars[,2])\nprint(cor)\n\n#&gt;        income\n#&gt; age 0.5479432\n\n\n\n\n\nCorrelation is easier to interpret because it is bounded with the magnitude showing us how strong or how weak is the relationship and the sign shows us the direction. Meanwhile the covariance is value that doesn’t really tell us much, in terms of the number provided.\n\n\n\n\n\\(E[income \\mid age &gt;= 18]\\)\n\n\ne1 &lt;- summarise(random_vars[random_vars$age&gt;=18,], mean = mean(income))\nprint(e1)\n\n#&gt; # A tibble: 1 × 1\n#&gt;    mean\n#&gt;   &lt;dbl&gt;\n#&gt; 1 4464.\n\n\n\n\\(E[income \\mid age \\in [18,65)]\\)\n\n\ne2 &lt;- summarise(random_vars[random_vars$age &gt;= 18 & random_vars$age&lt;65,], mean = mean(income))\nprint(e2)\n\n#&gt; # A tibble: 1 × 1\n#&gt;    mean\n#&gt;   &lt;dbl&gt;\n#&gt; 1 4686.\n\n\n\n\\(E[income \\mid age &gt;= 65]\\)\n\n\ne3 &lt;- summarise(random_vars[random_vars$age &gt;= 65,], mean = mean(income))\nprint(e3)\n\n#&gt; # A tibble: 1 × 1\n#&gt;    mean\n#&gt;   &lt;dbl&gt;\n#&gt; 1 1777."
  },
  {
    "objectID": "content/01_journal/04_causality.html",
    "href": "content/01_journal/04_causality.html",
    "title": "Causality",
    "section": "",
    "text": "Plotting a Spurious Correlation\nThis is a plot of one of the spurious correlations that relates US spending on science, space and technology to Suicides by hanging, strangulation and suffocation. I couldn’t find the data as a data set so I generated similar data to plot the graphs.\n\nlibrary(tidyverse)\nlibrary(patchwork)\n\n#Number of entries \nn &lt;- 20\n\n# Create Hanging Suicides Data\nhanging_suicide &lt;- tibble(\n  y = rnorm(n, exp(seq(from = 2, to = 4, length.out = n)), sd = 1),\n  x = seq(from = 1999, to = 2009, length.out = n)\n)\n\n#Creating US spending data\nUS_spending &lt;- tibble(\n  y1 = rnorm(n, exp(seq(from = 2, to = 4, length.out = n)), sd = 3),\n  x1 = seq(from = 1999, to = 2009, length.out = n)\n)\n\np1 &lt;- ggplot(hanging_suicide, aes(x = x, y = y)) +\n  xlab(\"Years\")+\n  ylab(\"No. of suicides (x100)\")+\n  geom_point(size = 3, alpha = 0.8) +\n  geom_line(linewidth = 1, color = 'red')\n  \np2 &lt;- ggplot(US_spending, aes(x = x1, y = y1)) +\n  xlab(\"Years\")+\n  ylab(\"Money Spent (Billions)\")+\n  geom_point(size = 3, alpha = 0.8) +\n  geom_line(linewidth = 1, color = 'blue')\n\np1 + p2"
  },
  {
    "objectID": "content/01_journal/09_iv.html",
    "href": "content/01_journal/09_iv.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "library(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.3     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dagitty)\nlibrary(ggdag)\n\n#&gt; \n#&gt; Attaching package: 'ggdag'\n#&gt; \n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n\nlibrary(estimatr)\n\napp &lt;- readRDS(\"~/Documents/TUHH/Casual Data Science for Business Analytics/Causal_Data_Science_Data/rand_enc.rds\")\n\napp\n\n\n\n  \n\n\n\n\n\n\n\napp_model &lt;- dagify(\n  F ~ P,\n  F ~ U,\n  S ~ F,\n  S ~ U,\n  coords = list(x = c(P = 1,F = 3,U = 5.5,S = 8),\n                y = c(P = 0,F = 0,U = 1.5,S = 0)),\n  labels = list(P = \"PopUp / Encouragement\",\n                F = \"New Feature\",\n                U = \"Unobserved Confounder\",\n                S = \"Screen Time\")\n)\n\n#Plot DAG model\nggdag(app_model) +\n  theme_dag()+\n  geom_dag_node(color = \"red\")+\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"black\") +\n  geom_dag_label_repel(aes(label = label))\n\n\n\n\n\n\n\n\n\n\n\n\n#Naive Biased Estimate\n\nlm_biased &lt;- lm(time_spent ~ used_ftr, data = app)\nsummary(lm_biased)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = time_spent ~ used_ftr, data = app)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -20.4950  -3.5393   0.0158   3.5961  20.5051 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 18.86993    0.06955   271.3   &lt;2e-16 ***\n#&gt; used_ftr    10.82269    0.10888    99.4   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 5.351 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.497,  Adjusted R-squared:  0.497 \n#&gt; F-statistic:  9881 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\napp_assume &lt;- app %&gt;%\n  filter(used_ftr == 1)\n\ncor(app_assume$rand_enc,app_assume$time_spent)\n\n#&gt; [1] -0.02744462\n\n\n\n\nThe value of the correlation between the IV variable and the outcome given the treatment is very small at -0.0274 which is not exactly zero but very small hence it is appropriate to use it specially that we most probably have a lot of unobserved confounders that can’t be all compensated/accounted for with a single IV variable.\n\n\n\n\n\nmodel_iv &lt;- iv_robust(time_spent ~ used_ftr | rand_enc, data = app)\nsummary(model_iv)\n\n#&gt; \n#&gt; Call:\n#&gt; iv_robust(formula = time_spent ~ used_ftr | rand_enc, data = app)\n#&gt; \n#&gt; Standard error type:  HC2 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value  Pr(&gt;|t|) CI Lower CI Upper   DF\n#&gt; (Intercept)   19.312     0.2248   85.89 0.000e+00   18.872    19.75 9998\n#&gt; used_ftr       9.738     0.5353   18.19 8.716e-73    8.689    10.79 9998\n#&gt; \n#&gt; Multiple R-squared:  0.4921 ,    Adjusted R-squared:  0.492 \n#&gt; F-statistic:   331 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nThe naive estimate appears to be biased yes with a bit of upward bias as the naive estimate is higher than the IV estimate."
  },
  {
    "objectID": "content/01_journal/07_matching.html",
    "href": "content/01_journal/07_matching.html",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "library(tidyverse)\nlibrary(patchwork)\nlibrary(dagitty)\nlibrary(ggdag)\nlibrary(ggplot2)\nlibrary(MatchIt)\n\nstore &lt;- readRDS(\"~/Documents/TUHH/Casual Data Science for Business Analytics/Causal_Data_Science_Data/membership.rds\")\n\nstore\n\n\n\n  \n\n\nstore_model &lt;- dagify(\n  M ~ A,\n  M ~ S,\n  M ~ P,\n  R ~ A,\n  R ~ S,\n  R ~ P,\n  coords = list(x = c(A = 1,S = 4,P = 7,M = 1,R = 7),\n                y = c(A = 2,S = 2,P = 2,M = 0,R = 0)),\n  labels = list(A = \"Age\",\n                P = \"Previousverage Purchase\",\n                S = \"Sex\",\n                M = \"Membership\",\n                R = \"Revenue\")\n)\n\n#Plot DAG model\nggdag(store_model) +\n  theme_dag()+\n  geom_dag_node(color = \"red\")+\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"black\") +\n  geom_dag_label_repel(aes(label = label))\n\n\n\n\n\n\n\n\n\n\n\n\nnaive_lm &lt;- lm(avg_purch ~ card, data = store)\nsummary(naive_lm)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = store)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -101.515  -20.684   -0.199   20.424  120.166 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  65.9397     0.3965  166.29   &lt;2e-16 ***\n#&gt; card         25.2195     0.6095   41.38   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.11 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.1462, Adjusted R-squared:  0.1461 \n#&gt; F-statistic:  1712 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\ncem &lt;- matchit(card  ~ avg_purch + age + sex + pre_avg_purch,\n               data = store, \n               method = 'cem', \n               estimand = 'ATE')\n\ndf_cem &lt;- match.data(cem)\n\ncem_lm &lt;- lm(avg_purch ~ card, data = df_cem, weights = weights)\nsummary(cem_lm)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_cem, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -162.422  -19.058    0.153   18.795  147.646 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  75.9624     0.4051 187.526   &lt;2e-16 ***\n#&gt; card          0.9485     0.6212   1.527    0.127    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 28.34 on 8515 degrees of freedom\n#&gt; Multiple R-squared:  0.0002737,  Adjusted R-squared:  0.0001563 \n#&gt; F-statistic: 2.332 on 1 and 8515 DF,  p-value: 0.1268\n\n\n\n\n\n\nnn &lt;- matchit(card  ~ avg_purch + age + sex + pre_avg_purch,\n              data = store,\n              method = \"nearest\", # changed\n              distance = \"mahalanobis\", # changed\n              replace = T)\n\ndf_nn &lt;- match.data(nn)\n\nnn_lm &lt;- lm(avg_purch ~ card, data = df_nn, weights = weights)\nsummary(nn_lm)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_nn, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -101.515  -22.370   -3.279   16.475  178.481 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  90.2268     0.6438 140.155   &lt;2e-16 ***\n#&gt; card          0.9324     0.7939   1.174     0.24    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.22 on 6434 degrees of freedom\n#&gt; Multiple R-squared:  0.0002143,  Adjusted R-squared:  5.895e-05 \n#&gt; F-statistic: 1.379 on 1 and 6434 DF,  p-value: 0.2403\n\n\n\n\n\n\nmodel_prop &lt;- glm(card  ~ avg_purch + age + sex + pre_avg_purch,\n                  data = store,\n                  family = binomial(link = \"logit\"))\n\n# Add propensities to table\ndf_aug &lt;- store %&gt;% mutate(propensity = predict(model_prop, type = \"response\"))\n\n# Extend data by IPW scores\ndf_ipw &lt;- df_aug %&gt;% mutate(\n  ipw = (card/propensity) + ((1-card) / (1-propensity)))\n\n#Estimation\nipw_lm &lt;- lm(avg_purch ~ card,\n                data = df_ipw, \n                weights = ipw)\nsummary(ipw_lm)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_ipw, weights = ipw)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -525.64  -28.55   -0.11   30.04  441.02 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  76.5744     0.4664 164.189   &lt;2e-16 ***\n#&gt; card         -0.6361     0.6577  -0.967    0.333    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 46.6 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  9.356e-05,  Adjusted R-squared:  -6.45e-06 \n#&gt; F-statistic: 0.9355 on 1 and 9998 DF,  p-value: 0.3335"
  },
  {
    "objectID": "content/01_journal/05_dag.html",
    "href": "content/01_journal/05_dag.html",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "In the parking spots example we had the parking spots as treatment variables, the stores as observation units and the sales as an outcome also taking into consideration the location as a confounder.\n\n# Load packages\nlibrary(dagitty)\nlibrary(ggdag)\n\n#&gt; \n#&gt; Attaching package: 'ggdag'\n\n\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n\nlibrary(ggplot2)\n\n#Create DAG model\nparking_model &lt;- dagify(\n  S ~ L,\n  A ~ P,\n  B ~ P,\n  C ~ P,\n  D ~ P,\n  S ~ A,\n  S ~ B,\n  S ~ C,\n  S ~ D,\n  coords = list(x = c(L = 7,P = 1,A = 3,B = 3,C = 3,D = 3,S = 7),\n                y = c(L = 1,P = 0,A = 1,B = 0.5,C = -0.5,D = -1,S = 0)),\n  labels = list(L=\"location\",\n                P = \"parking spots\",\n                A = \"Store A\",\n                B = \"Store B\",\n                C = \"Store C\",\n                D = \"Store D\",\n                S = \"Sales\")\n)\n\n#Plot DAG model\nggdag(parking_model) +\n  theme_dag()+\n  geom_dag_node(color = \"red\")+\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"black\") +\n  geom_dag_label_repel(aes(label = label))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.3     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#&gt; ✔ lubridate 1.9.3     ✔ tibble    3.2.1\n#&gt; ✔ purrr     1.0.2     ✔ tidyr     1.3.0\n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks ggdag::filter(), stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ncustomer_sat &lt;- readRDS(\"~/Documents/TUHH/Casual Data Science for Business Analytics/Causal_Data_Science_Data/customer_sat.rds\")\nhead(customer_sat)\n\n\n\n  \n\n\nlm_first &lt;- lm(satisfaction ~ follow_ups, data = customer_sat)\nsummary(lm_first)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups, data = customer_sat)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -12.412  -5.257   1.733   4.506  12.588 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  78.8860     4.2717  18.467 1.04e-10 ***\n#&gt; follow_ups   -3.3093     0.6618  -5.001 0.000243 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 7.923 on 13 degrees of freedom\n#&gt; Multiple R-squared:  0.658,  Adjusted R-squared:  0.6316 \n#&gt; F-statistic: 25.01 on 1 and 13 DF,  p-value: 0.0002427\n\nlm_second &lt;- lm(satisfaction ~ follow_ups + subscription, data = customer_sat)\nsummary(lm_second)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups + subscription, data = customer_sat)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -4.3222 -2.1972  0.3167  2.2667  3.9944 \n#&gt; \n#&gt; Coefficients:\n#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)           26.7667     6.6804   4.007  0.00206 ** \n#&gt; follow_ups             2.1944     0.7795   2.815  0.01682 *  \n#&gt; subscriptionPremium   44.7222     5.6213   7.956 6.88e-06 ***\n#&gt; subscriptionPremium+  18.0722     2.1659   8.344 4.37e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.958 on 11 degrees of freedom\n#&gt; Multiple R-squared:  0.9597, Adjusted R-squared:  0.9487 \n#&gt; F-statistic: 87.21 on 3 and 11 DF,  p-value: 5.956e-08\n\n\n\n\n\nOne of the possible reasons for having a negative estimate in the first regression and then a positive one in the second could be due to the nature of the follow-up calls. So when we speak about follow-ups in general its usually a problem and at lower subscription levels the solution could be to upgrade their subscription which is more expensive and hence the customer isn’t satisfied with the outcome, it could also be a bug or a complain which applies to all subscription levels but when you factor in the subscription level the nature of the calls could be about an upgrade or an inquiry about a feature that gets resolved and the customer is satisfied.\nAnother explanation that appears later when plotting the data is that when we don’t factor the subscription levels it shows that overall the more the follow-ups the less the satisfaction but when you factor the subscription you find that every subscription level has its own range of satisfaction which when isolated shows us an increase in satisfaction as the increase of follow-ups.\n\n\n\n\nno_sub_sat &lt;- ggplot(customer_sat, aes(x = follow_ups, y = satisfaction)) +\n  geom_point(alpha = .8) +\n  stat_smooth(method = \"lm\", se = F)\n\nsub_sat &lt;- ggplot(customer_sat, aes(x = follow_ups, y = satisfaction, color = subscription)) +\n  geom_point(alpha = .8) +\n  stat_smooth(method = \"lm\", se = F)\n\nno_sub_sat\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nsub_sat\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "content/01_journal/06_rct.html",
    "href": "content/01_journal/06_rct.html",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "library(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.3     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nabtest &lt;- readRDS(\"~/Documents/TUHH/Casual Data Science for Business Analytics/Causal_Data_Science_Data/abtest_online.rds\")\n\ncompare &lt;- \n  ggplot(abtest, \n         aes(x = chatbot, \n             y = purchase_amount, \n             color = as.factor(chatbot))) +\n  stat_summary(geom = \"errorbar\", \n               width = .5,\n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"Purchase Amount\", title = \"Difference in Purchase Amount\")\n\ncompare\n\n\n\n\n\n\n\n\nAs we can see in the plot the covariates are not balanced.\n\n\n\n\nlm_sales &lt;- lm(purchase_amount ~ chatbot, data = abtest)\nsummary(lm_sales)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot, data = abtest)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -16.702 -14.478  -9.626  13.922  64.648 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  16.7017     0.8374  19.944  &lt; 2e-16 ***\n#&gt; chatbotTRUE  -7.0756     1.1796  -5.998 2.79e-09 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.65 on 998 degrees of freedom\n#&gt; Multiple R-squared:  0.0348, Adjusted R-squared:  0.03383 \n#&gt; F-statistic: 35.98 on 1 and 998 DF,  p-value: 2.787e-09\n\n\nThe coefficient is a negative number telling us that the chatbots are affecting the sales in a negative way decreasing the sales and not increasing them.\n\n\n\n\nlm_sales_plus_mob &lt;- lm(purchase_amount ~ chatbot*mobile_device, data = abtest)\nsummary(lm_sales_plus_mob)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot * mobile_device, data = abtest)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -16.98 -14.54  -9.95  14.13  65.24 \n#&gt; \n#&gt; Coefficients:\n#&gt;                               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)                    16.9797     1.0152  16.725   &lt;2e-16 ***\n#&gt; chatbotTRUE                    -7.0301     1.4284  -4.922    1e-06 ***\n#&gt; mobile_deviceTRUE              -0.8727     1.7987  -0.485    0.628    \n#&gt; chatbotTRUE:mobile_deviceTRUE  -0.1526     2.5369  -0.060    0.952    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.66 on 996 degrees of freedom\n#&gt; Multiple R-squared:  0.03534,    Adjusted R-squared:  0.03244 \n#&gt; F-statistic: 12.16 on 3 and 996 DF,  p-value: 8.034e-08\n\n\n\n\n\n\nglm_sales &lt;- glm(purchase ~ chatbot, family = binomial(link = 'logit'), abtest)\nsummary(glm_sales)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = purchase ~ chatbot, family = binomial(link = \"logit\"), \n#&gt;     data = abtest)\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept) -0.01613    0.08981  -0.180    0.857    \n#&gt; chatbotTRUE -0.98939    0.13484  -7.337 2.18e-13 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 1329.1  on 999  degrees of freedom\n#&gt; Residual deviance: 1273.3  on 998  degrees of freedom\n#&gt; AIC: 1277.3\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\n\nIn logistic regression we don’t just look at the coefficient but its exponent and that tells us the “odds ratio” which is the ratio of the probability of an event happening divided by the probability of the non event.\nSince the coefficient here is a negative number hence the odds ratio is less than one and that tells us that the event will most likely not happen.\nIn our case the event is the purchase and so the regression tells us that having the chatbot means that the probability of the customer making a purchase is less than that when the customer doesn’t have a chat bot. This means that the chat bot is negatively affecting the number of purchases and hence decreasing the sales."
  },
  {
    "objectID": "content/01_journal/08_did.html",
    "href": "content/01_journal/08_did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "library(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.3     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nhospitals &lt;- readRDS(\"~/Documents/TUHH/Casual Data Science for Business Analytics/Causal_Data_Science_Data/hospdd.rds\")\n\n\n\n\n\n# Step 1: Difference between treatment and control group BEFORE treatment\nbefore_control &lt;- hospitals %&gt;%\n  filter(hospital &gt;= 19 & hospital&lt;= 46, procedure == 0) %&gt;% \n  pull(satis)\nbefore_treatment &lt;- hospitals %&gt;%\n  filter(hospital &gt;= 1 & hospital&lt;=18, procedure == 0) %&gt;% \n  pull(satis)\n\ndiff_before &lt;- before_treatment - before_control\n\n#&gt; Warning in before_treatment - before_control: longer object length is not a\n#&gt; multiple of shorter object length\n\n# Step 2: Difference between treatment and control group AFTER treatment\nafter_control &lt;- rep(0, dim(hospitals)[1])\nafter_treatment &lt;- hospitals %&gt;%\n  filter(hospital &gt;= 1 & hospital&lt;=18, procedure == 1) %&gt;% \n  pull(satis)\n\ndiff_after &lt;- after_treatment - after_control\n\n#&gt; Warning in after_treatment - after_control: longer object length is not a\n#&gt; multiple of shorter object length\n\ndiff_diff &lt;- diff_after - diff_before\n\n#&gt; Warning in diff_after - diff_before: longer object length is not a multiple of\n#&gt; shorter object length\n\n# sprintf(\"Estimate: %.2f\", diff_diff)\n# didn't print it cause its very wrong\n\n\n\n\n\n\n\nlm_1 &lt;- lm(satis ~ month + hospital, data = hospitals)\nsummary(lm_1)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satis ~ month + hospital, data = hospitals)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -3.3831 -0.6724 -0.0838  0.5778  5.7881 \n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  3.7597212  0.0308145  122.01   &lt;2e-16 ***\n#&gt; month        0.0720728  0.0055957   12.88   &lt;2e-16 ***\n#&gt; hospital    -0.0175982  0.0008732  -20.16   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 1.017 on 7365 degrees of freedom\n#&gt; Multiple R-squared:  0.07208,    Adjusted R-squared:  0.07183 \n#&gt; F-statistic: 286.1 on 2 and 7365 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\nlm_2 &lt;- lm(satis ~ as.factor(month) + as.factor(hospital), data = hospitals)\nsummary(lm_2)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satis ~ as.factor(month) + as.factor(hospital), \n#&gt;     data = hospitals)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -3.4357 -0.4930 -0.0120  0.4755  4.5398 \n#&gt; \n#&gt; Coefficients:\n#&gt;                        Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)            3.419332   0.057597  59.367  &lt; 2e-16 ***\n#&gt; as.factor(month)2     -0.009608   0.030411  -0.316 0.752069    \n#&gt; as.factor(month)3      0.021969   0.030411   0.722 0.470083    \n#&gt; as.factor(month)4      0.349354   0.030411  11.488  &lt; 2e-16 ***\n#&gt; as.factor(month)5      0.343235   0.030411  11.286  &lt; 2e-16 ***\n#&gt; as.factor(month)6      0.348800   0.030411  11.469  &lt; 2e-16 ***\n#&gt; as.factor(month)7      0.341444   0.030411  11.228  &lt; 2e-16 ***\n#&gt; as.factor(hospital)2   0.408566   0.080413   5.081 3.85e-07 ***\n#&gt; as.factor(hospital)3   0.533625   0.082596   6.461 1.11e-10 ***\n#&gt; as.factor(hospital)4   0.227510   0.076977   2.956 0.003131 ** \n#&gt; as.factor(hospital)5  -0.145353   0.076977  -1.888 0.059030 .  \n#&gt; as.factor(hospital)6   0.447863   0.076977   5.818 6.20e-09 ***\n#&gt; as.factor(hospital)7   1.404416   0.074390  18.879  &lt; 2e-16 ***\n#&gt; as.factor(hospital)8   0.071876   0.079452   0.905 0.365685    \n#&gt; as.factor(hospital)9  -1.518515   0.081457 -18.642  &lt; 2e-16 ***\n#&gt; as.factor(hospital)10  1.682845   0.080413  20.927  &lt; 2e-16 ***\n#&gt; as.factor(hospital)11  0.220965   0.079452   2.781 0.005431 ** \n#&gt; as.factor(hospital)12 -0.095303   0.081457  -1.170 0.242047    \n#&gt; as.factor(hospital)13  0.495593   0.078564   6.308 2.99e-10 ***\n#&gt; as.factor(hospital)14  0.233043   0.082596   2.821 0.004793 ** \n#&gt; as.factor(hospital)15 -0.144494   0.082596  -1.749 0.080263 .  \n#&gt; as.factor(hospital)16  1.414268   0.080413  17.588  &lt; 2e-16 ***\n#&gt; as.factor(hospital)17  0.423543   0.083843   5.052 4.49e-07 ***\n#&gt; as.factor(hospital)18  0.153276   0.097668   1.569 0.116609    \n#&gt; as.factor(hospital)19 -1.169296   0.082596 -14.157  &lt; 2e-16 ***\n#&gt; as.factor(hospital)20 -0.376607   0.080413  -4.683 2.87e-06 ***\n#&gt; as.factor(hospital)21  0.770343   0.085215   9.040  &lt; 2e-16 ***\n#&gt; as.factor(hospital)22  0.375321   0.083843   4.476 7.70e-06 ***\n#&gt; as.factor(hospital)23  0.277726   0.082596   3.362 0.000776 ***\n#&gt; as.factor(hospital)24 -0.732120   0.088421  -8.280  &lt; 2e-16 ***\n#&gt; as.factor(hospital)25  0.222480   0.094875   2.345 0.019055 *  \n#&gt; as.factor(hospital)26 -0.209747   0.080413  -2.608 0.009116 ** \n#&gt; as.factor(hospital)27 -0.822648   0.077742 -10.582  &lt; 2e-16 ***\n#&gt; as.factor(hospital)28  0.288001   0.085215   3.380 0.000729 ***\n#&gt; as.factor(hospital)29 -0.175443   0.081457  -2.154 0.031288 *  \n#&gt; as.factor(hospital)30 -0.591916   0.097668  -6.060 1.42e-09 ***\n#&gt; as.factor(hospital)31  0.088091   0.080413   1.095 0.273344    \n#&gt; as.factor(hospital)32 -0.747340   0.081457  -9.175  &lt; 2e-16 ***\n#&gt; as.factor(hospital)33 -0.877969   0.080413 -10.918  &lt; 2e-16 ***\n#&gt; as.factor(hospital)34 -0.424406   0.075599  -5.614 2.05e-08 ***\n#&gt; as.factor(hospital)35 -0.069883   0.077742  -0.899 0.368729    \n#&gt; as.factor(hospital)36  1.714149   0.078564  21.818  &lt; 2e-16 ***\n#&gt; as.factor(hospital)37 -0.283590   0.094875  -2.989 0.002807 ** \n#&gt; as.factor(hospital)38 -0.510800   0.079452  -6.429 1.36e-10 ***\n#&gt; as.factor(hospital)39 -0.447491   0.083843  -5.337 9.72e-08 ***\n#&gt; as.factor(hospital)40  0.697539   0.079452   8.779  &lt; 2e-16 ***\n#&gt; as.factor(hospital)41 -0.573729   0.077742  -7.380 1.76e-13 ***\n#&gt; as.factor(hospital)42  0.457143   0.086733   5.271 1.40e-07 ***\n#&gt; as.factor(hospital)43 -1.196426   0.082596 -14.485  &lt; 2e-16 ***\n#&gt; as.factor(hospital)44 -0.389582   0.092446  -4.214 2.54e-05 ***\n#&gt; as.factor(hospital)45 -0.637743   0.077742  -8.203 2.74e-16 ***\n#&gt; as.factor(hospital)46 -0.345502   0.083843  -4.121 3.82e-05 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.7536 on 7316 degrees of freedom\n#&gt; Multiple R-squared:  0.4941, Adjusted R-squared:  0.4905 \n#&gt; F-statistic: 140.1 on 51 and 7316 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nI believe I will use the second one with as.factor as it provides estimates for each hospital and also each month rather than all hospitals in all months in the first one."
  },
  {
    "objectID": "content/01_journal/01_probability.html",
    "href": "content/01_journal/01_probability.html",
    "title": "Probability Theory",
    "section": "",
    "text": "\\(P(T \\cap S) = P(T \\mid S) * P(S) = 0.2 *0.3 = 0.06\\)\n\\(P(T \\cap \\overline{S}) = P(T \\mid \\overline{S}) * P(\\overline{S}) = 0.6 *0.7 = 0.42\\)\n\\(P(\\overline{T} \\cap S) = P(\\overline{T} \\mid S) * P(S) = 0.8 *0.3 = 0.24\\)\n\\(P(\\overline{T} \\cap \\overline{S}) = P(\\overline{T} \\mid \\overline{S}) * P(\\overline{S}) = 0.7 *0.4 = 0.28\\)"
  },
  {
    "objectID": "content/01_journal/01_probability.html#header-2",
    "href": "content/01_journal/01_probability.html#header-2",
    "title": "Probability Theory",
    "section": "3.1 Header 2",
    "text": "3.1 Header 2\n\nHeader 3\n\nHeader 4\n\nHeader 5\n\nHeader 6"
  },
  {
    "objectID": "content/01_journal/03_regression.html",
    "href": "content/01_journal/03_regression.html",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "library(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.3     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ncar_prices &lt;- readRDS(\"~/Documents/TUHH/Casual Data Science for Business Analytics/Causal_Data_Science_Data/car_prices.rds\")\n\ndim(car_prices)\n\n#&gt; [1] 181  22\n\n\nThe dimensions of the data set is 188 rows and 22 coloumns\n\n\n\n\nglimpse(car_prices)\n\n#&gt; Rows: 181\n#&gt; Columns: 22\n#&gt; $ aspiration       &lt;chr&gt; \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std…\n#&gt; $ doornumber       &lt;chr&gt; \"two\", \"two\", \"two\", \"four\", \"four\", \"two\", \"four\", \"…\n#&gt; $ carbody          &lt;chr&gt; \"convertible\", \"convertible\", \"hatchback\", \"sedan\", \"…\n#&gt; $ drivewheel       &lt;chr&gt; \"rwd\", \"rwd\", \"rwd\", \"fwd\", \"4wd\", \"fwd\", \"fwd\", \"fwd…\n#&gt; $ enginelocation   &lt;chr&gt; \"front\", \"front\", \"front\", \"front\", \"front\", \"front\",…\n#&gt; $ wheelbase        &lt;dbl&gt; 88.6, 88.6, 94.5, 99.8, 99.4, 99.8, 105.8, 105.8, 105…\n#&gt; $ carlength        &lt;dbl&gt; 168.8, 168.8, 171.2, 176.6, 176.6, 177.3, 192.7, 192.…\n#&gt; $ carwidth         &lt;dbl&gt; 64.1, 64.1, 65.5, 66.2, 66.4, 66.3, 71.4, 71.4, 71.4,…\n#&gt; $ carheight        &lt;dbl&gt; 48.8, 48.8, 52.4, 54.3, 54.3, 53.1, 55.7, 55.7, 55.9,…\n#&gt; $ curbweight       &lt;dbl&gt; 2548, 2548, 2823, 2337, 2824, 2507, 2844, 2954, 3086,…\n#&gt; $ enginetype       &lt;chr&gt; \"dohc\", \"dohc\", \"ohcv\", \"ohc\", \"ohc\", \"ohc\", \"ohc\", \"…\n#&gt; $ cylindernumber   &lt;chr&gt; \"four\", \"four\", \"six\", \"four\", \"five\", \"five\", \"five\"…\n#&gt; $ enginesize       &lt;dbl&gt; 130, 130, 152, 109, 136, 136, 136, 136, 131, 131, 108…\n#&gt; $ fuelsystem       &lt;chr&gt; \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi…\n#&gt; $ boreratio        &lt;dbl&gt; 3.47, 3.47, 2.68, 3.19, 3.19, 3.19, 3.19, 3.19, 3.13,…\n#&gt; $ stroke           &lt;dbl&gt; 2.68, 2.68, 3.47, 3.40, 3.40, 3.40, 3.40, 3.40, 3.40,…\n#&gt; $ compressionratio &lt;dbl&gt; 9.00, 9.00, 9.00, 10.00, 8.00, 8.50, 8.50, 8.50, 8.30…\n#&gt; $ horsepower       &lt;dbl&gt; 111, 111, 154, 102, 115, 110, 110, 110, 140, 160, 101…\n#&gt; $ peakrpm          &lt;dbl&gt; 5000, 5000, 5000, 5500, 5500, 5500, 5500, 5500, 5500,…\n#&gt; $ citympg          &lt;dbl&gt; 21, 21, 19, 24, 18, 19, 19, 19, 17, 16, 23, 23, 21, 2…\n#&gt; $ highwaympg       &lt;dbl&gt; 27, 27, 26, 30, 22, 25, 25, 25, 20, 22, 29, 29, 28, 2…\n#&gt; $ price            &lt;dbl&gt; 13495.00, 16500.00, 16500.00, 13950.00, 17450.00, 152…\n\n\nHere I used glimpse instead of head to help me see all the coloums and their data types. Strings are recorded as chars and numbers are double allowing decimal values.\n\n\n\n\nlm_all &lt;- lm(price ~ ., data = car_prices)\nsummary(lm_all)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ ., data = car_prices)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -5662  -1120      0    798   9040 \n#&gt; \n#&gt; Coefficients:\n#&gt;                        Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)          -36269.965  15460.866  -2.346 0.020354 *  \n#&gt; aspirationturbo        1846.206   1041.391   1.773 0.078386 .  \n#&gt; doornumbertwo           242.523    571.929   0.424 0.672172    \n#&gt; carbodyhardtop        -3691.743   1424.825  -2.591 0.010561 *  \n#&gt; carbodyhatchback      -3344.335   1238.359  -2.701 0.007757 ** \n#&gt; carbodysedan          -2292.820   1356.014  -1.691 0.093043 .  \n#&gt; carbodywagon          -3427.921   1490.285  -2.300 0.022885 *  \n#&gt; drivewheelfwd          -504.564   1076.623  -0.469 0.640030    \n#&gt; drivewheelrwd           -15.446   1268.070  -0.012 0.990299    \n#&gt; enginelocationrear     6643.492   2572.275   2.583 0.010806 *  \n#&gt; wheelbase               -30.197     92.776  -0.325 0.745294    \n#&gt; carlength               -29.740     51.672  -0.576 0.565824    \n#&gt; carwidth                731.819    244.533   2.993 0.003258 ** \n#&gt; carheight               123.195    134.607   0.915 0.361617    \n#&gt; curbweight                2.612      1.781   1.467 0.144706    \n#&gt; enginetypedohcv       -8541.957   4749.685  -1.798 0.074219 .  \n#&gt; enginetypel             978.748   1786.384   0.548 0.584619    \n#&gt; enginetypeohc          3345.252    933.001   3.585 0.000461 ***\n#&gt; enginetypeohcf          972.919   1625.631   0.598 0.550462    \n#&gt; enginetypeohcv        -6222.322   1236.415  -5.033 1.43e-06 ***\n#&gt; cylindernumberfive   -11724.540   3019.192  -3.883 0.000157 ***\n#&gt; cylindernumberfour   -11549.326   3177.177  -3.635 0.000387 ***\n#&gt; cylindernumbersix     -7151.398   2247.230  -3.182 0.001793 ** \n#&gt; cylindernumberthree   -4318.929   4688.833  -0.921 0.358545    \n#&gt; cylindernumbertwelve -11122.209   4196.494  -2.650 0.008946 ** \n#&gt; enginesize              125.934     26.541   4.745 5.00e-06 ***\n#&gt; fuelsystem2bbl          177.136    883.615   0.200 0.841400    \n#&gt; fuelsystemmfi         -3041.018   2576.996  -1.180 0.239934    \n#&gt; fuelsystemmpfi          359.278   1001.529   0.359 0.720326    \n#&gt; fuelsystemspdi        -2543.890   1363.546  -1.866 0.064140 .  \n#&gt; fuelsystemspfi          514.766   2499.229   0.206 0.837107    \n#&gt; boreratio             -1306.740   1642.221  -0.796 0.427516    \n#&gt; stroke                -4527.137    922.732  -4.906 2.49e-06 ***\n#&gt; compressionratio       -737.901    555.960  -1.327 0.186539    \n#&gt; horsepower               10.293     22.709   0.453 0.651035    \n#&gt; peakrpm                   2.526      0.634   3.983 0.000108 ***\n#&gt; citympg                 -90.352    166.647  -0.542 0.588538    \n#&gt; highwaympg              154.858    167.148   0.926 0.355761    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2189 on 143 degrees of freedom\n#&gt; Multiple R-squared:  0.9415, Adjusted R-squared:  0.9264 \n#&gt; F-statistic: 62.21 on 37 and 143 DF,  p-value: &lt; 2.2e-16\n\n\nFrom the summary we could see that the main factors relevant are:\n\nenginetype (mostly)\neniginesize\nstroke\npeakrpm\n\n\n\n\n\nlm_top &lt;- lm(price ~ enginetype +\n                     enginesize +\n                     stroke +\n                     peakrpm,\n                     data = car_prices)\nsummary(lm_top)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ enginetype + enginesize + stroke + peakrpm, \n#&gt;     data = car_prices)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -14872.1  -1453.3   -410.3   1563.4  10058.4 \n#&gt; \n#&gt; Coefficients:\n#&gt;                   Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)     -1.618e+04  4.220e+03  -3.834 0.000177 ***\n#&gt; enginetypedohcv  1.280e+03  3.135e+03   0.408 0.683638    \n#&gt; enginetypel      3.842e+03  1.480e+03   2.596 0.010256 *  \n#&gt; enginetypeohc    2.550e+03  9.641e+02   2.645 0.008925 ** \n#&gt; enginetypeohcf   6.534e+02  1.283e+03   0.509 0.611213    \n#&gt; enginetypeohcv  -4.967e+03  1.284e+03  -3.870 0.000154 ***\n#&gt; enginesize       2.145e+02  7.630e+00  28.106  &lt; 2e-16 ***\n#&gt; stroke          -5.398e+03  9.088e+02  -5.939 1.55e-08 ***\n#&gt; peakrpm          3.400e+00  5.522e-01   6.158 5.05e-09 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2971 on 172 degrees of freedom\n#&gt; Multiple R-squared:  0.8704, Adjusted R-squared:  0.8644 \n#&gt; F-statistic: 144.4 on 8 and 172 DF,  p-value: &lt; 2.2e-16\n\n\nAs we can see the smallest p-value is the one for engine size and hence im picking it to be my regressor.\n\nmax(car_prices$enginesize)\n\n#&gt; [1] 326\n\nmin(car_prices$enginesize)\n\n#&gt; [1] 61\n\n\nThe regressor I chose is of type double and its values in this data set ranges from 61 to 326.\nIt has a big effect on the price since the p value is very small it denies the null hypothesis and tells that that there is a strong corellation between both factors.\n\nlm_summarised &lt;- lm(price ~ enginesize, data = car_prices)\nconfint(lm_summarised, level = 0.95)\n\n#&gt;                   2.5 %     97.5 %\n#&gt; (Intercept) -10346.0474 -6898.5441\n#&gt; enginesize     157.1932   182.9353\n\n\nThe estimate is completely positive hence the effect is statistically significant.\n\n\n\n\nnew_car_prices &lt;- mutate(car_prices, seat_heating = TRUE)\nlm_heat &lt;- lm(price ~ ., data = new_car_prices)\nsummary(lm_heat)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ ., data = new_car_prices)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -5662  -1120      0    798   9040 \n#&gt; \n#&gt; Coefficients: (1 not defined because of singularities)\n#&gt;                        Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)          -36269.965  15460.866  -2.346 0.020354 *  \n#&gt; aspirationturbo        1846.206   1041.391   1.773 0.078386 .  \n#&gt; doornumbertwo           242.523    571.929   0.424 0.672172    \n#&gt; carbodyhardtop        -3691.743   1424.825  -2.591 0.010561 *  \n#&gt; carbodyhatchback      -3344.335   1238.359  -2.701 0.007757 ** \n#&gt; carbodysedan          -2292.820   1356.014  -1.691 0.093043 .  \n#&gt; carbodywagon          -3427.921   1490.285  -2.300 0.022885 *  \n#&gt; drivewheelfwd          -504.564   1076.623  -0.469 0.640030    \n#&gt; drivewheelrwd           -15.446   1268.070  -0.012 0.990299    \n#&gt; enginelocationrear     6643.492   2572.275   2.583 0.010806 *  \n#&gt; wheelbase               -30.197     92.776  -0.325 0.745294    \n#&gt; carlength               -29.740     51.672  -0.576 0.565824    \n#&gt; carwidth                731.819    244.533   2.993 0.003258 ** \n#&gt; carheight               123.195    134.607   0.915 0.361617    \n#&gt; curbweight                2.612      1.781   1.467 0.144706    \n#&gt; enginetypedohcv       -8541.957   4749.685  -1.798 0.074219 .  \n#&gt; enginetypel             978.748   1786.384   0.548 0.584619    \n#&gt; enginetypeohc          3345.252    933.001   3.585 0.000461 ***\n#&gt; enginetypeohcf          972.919   1625.631   0.598 0.550462    \n#&gt; enginetypeohcv        -6222.322   1236.415  -5.033 1.43e-06 ***\n#&gt; cylindernumberfive   -11724.540   3019.192  -3.883 0.000157 ***\n#&gt; cylindernumberfour   -11549.326   3177.177  -3.635 0.000387 ***\n#&gt; cylindernumbersix     -7151.398   2247.230  -3.182 0.001793 ** \n#&gt; cylindernumberthree   -4318.929   4688.833  -0.921 0.358545    \n#&gt; cylindernumbertwelve -11122.209   4196.494  -2.650 0.008946 ** \n#&gt; enginesize              125.934     26.541   4.745 5.00e-06 ***\n#&gt; fuelsystem2bbl          177.136    883.615   0.200 0.841400    \n#&gt; fuelsystemmfi         -3041.018   2576.996  -1.180 0.239934    \n#&gt; fuelsystemmpfi          359.278   1001.529   0.359 0.720326    \n#&gt; fuelsystemspdi        -2543.890   1363.546  -1.866 0.064140 .  \n#&gt; fuelsystemspfi          514.766   2499.229   0.206 0.837107    \n#&gt; boreratio             -1306.740   1642.221  -0.796 0.427516    \n#&gt; stroke                -4527.137    922.732  -4.906 2.49e-06 ***\n#&gt; compressionratio       -737.901    555.960  -1.327 0.186539    \n#&gt; horsepower               10.293     22.709   0.453 0.651035    \n#&gt; peakrpm                   2.526      0.634   3.983 0.000108 ***\n#&gt; citympg                 -90.352    166.647  -0.542 0.588538    \n#&gt; highwaympg              154.858    167.148   0.926 0.355761    \n#&gt; seat_heatingTRUE             NA         NA      NA       NA    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2189 on 143 degrees of freedom\n#&gt; Multiple R-squared:  0.9415, Adjusted R-squared:  0.9264 \n#&gt; F-statistic: 62.21 on 37 and 143 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/10_rdd.html",
    "href": "content/01_journal/10_rdd.html",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "library(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.3     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rddensity)\n\ndf &lt;- readRDS(\"~/Documents/TUHH/Casual Data Science for Business Analytics/Causal_Data_Science_Data/coupon.rds\")\ndf\n\n\n\n  \n\n\n# Define cut-off\nc0 &lt;- 60\n\n# Define all 3 bandwidths\nbw &lt;- c0 + c(-5, 5)\nbw_1 &lt;- c0 + c(-2.5,2,5) # Half\nbw_2 &lt;- c0 + c(-10,10)   # Double\n\n\n\n\n\n# Subsets below and above threshold in specified bandwidth\ndf_bw_below &lt;- df %&gt;% filter(days_since_last &gt;= bw[1] & days_since_last &lt; c0)\ndf_bw_above &lt;- df %&gt;% filter(days_since_last &gt;= c0 & days_since_last &lt;= bw[2])\n\ndf_bw &lt;- bind_rows(df_bw_above, df_bw_below)\n\nlm_bw &lt;- lm(purchase_after ~ days_since_last_centered + coupon, df_bw)\nsummary(lm_bw)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_after ~ days_since_last_centered + coupon, \n#&gt;     data = df_bw)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -11.4966  -2.1312  -0.0949   2.0185  10.4159 \n#&gt; \n#&gt; Coefficients:\n#&gt;                          Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)               11.4242     0.3965  28.813  &lt; 2e-16 ***\n#&gt; days_since_last_centered   0.3835     0.1259   3.046  0.00251 ** \n#&gt; couponTRUE                 7.9334     0.7087  11.194  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.186 on 320 degrees of freedom\n#&gt; Multiple R-squared:  0.7074, Adjusted R-squared:  0.7055 \n#&gt; F-statistic: 386.8 on 2 and 320 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n# Case of Half the bandwidth\ndf_bw_below_1 &lt;- df %&gt;% filter(days_since_last &gt;= bw_1[1] & days_since_last &lt; c0)\ndf_bw_above_1 &lt;- df %&gt;% filter(days_since_last &gt;= c0 & days_since_last &lt;= bw_1[2])\n\ndf_bw_1 &lt;- bind_rows(df_bw_above_1, df_bw_below_1)\n\nlm_bw_1 &lt;- lm(purchase_after ~ days_since_last_centered + coupon, df_bw_1)\nsummary(lm_bw_1)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_after ~ days_since_last_centered + coupon, \n#&gt;     data = df_bw_1)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -10.9756  -2.1067  -0.0376   2.0936   8.0708 \n#&gt; \n#&gt; Coefficients:\n#&gt;                          Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)               11.3543     0.6438  17.636  &lt; 2e-16 ***\n#&gt; days_since_last_centered   0.4740     0.3774   1.256    0.211    \n#&gt; couponTRUE                 7.4823     1.0771   6.947 9.38e-11 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.352 on 157 degrees of freedom\n#&gt; Multiple R-squared:  0.6255, Adjusted R-squared:  0.6208 \n#&gt; F-statistic: 131.1 on 2 and 157 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n# Case of Double the bandwidth\ndf_bw_below_2 &lt;- df %&gt;% filter(days_since_last &gt;= bw_2[1] & days_since_last &lt; c0)\ndf_bw_above_2 &lt;- df %&gt;% filter(days_since_last &gt;= c0 & days_since_last &lt;= bw_2[2])\n\ndf_bw_2 &lt;- bind_rows(df_bw_above_2, df_bw_below_2)\n\nlm_bw_2 &lt;- lm(purchase_after ~ days_since_last_centered + coupon, df_bw_2)\nsummary(lm_bw_2)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_after ~ days_since_last_centered + coupon, \n#&gt;     data = df_bw_2)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -12.2718  -2.0858  -0.0003   2.0275  10.6749 \n#&gt; \n#&gt; Coefficients:\n#&gt;                          Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)              10.61700    0.27386  38.767   &lt;2e-16 ***\n#&gt; days_since_last_centered  0.01413    0.04255   0.332     0.74    \n#&gt; couponTRUE                9.51584    0.48628  19.569   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.115 on 626 degrees of freedom\n#&gt; Multiple R-squared:  0.7052, Adjusted R-squared:  0.7042 \n#&gt; F-statistic: 748.6 on 2 and 626 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nThe bandwidth indeed affect the estimate, well at first it didn’t appear like it when we used half the bandwidth only but in the second case of double the original bandwidth we saw a significant change and this tells us that depending on our bandwidth and number of cases within that bandwidth we may get varying results so its important to choose the right bandwidth according to theoretical knowledge of the project which would help us yield better results. I beleive it is one of the parameters that would vary differently from cases to case and must be tuned carefully."
  },
  {
    "objectID": "content/01_journal/01_probability.html#givens",
    "href": "content/01_journal/01_probability.html#givens",
    "title": "Probability Theory",
    "section": "Givens",
    "text": "Givens\n\\(P(B \\mid A) = 0.97\\)\n\\(P(B \\mid \\overline{A}) = 0.01\\)\n\\(P(A) = 0.04\\)"
  },
  {
    "objectID": "content/01_journal/01_probability.html#solution",
    "href": "content/01_journal/01_probability.html#solution",
    "title": "Probability Theory",
    "section": "",
    "text": "\\(P(T \\cap S) = P(T \\mid S) * P(S) = 0.2 *0.3 = 0.06\\)\n\\(P(T \\cap \\overline{S}) = P(T \\mid \\overline{S}) * P(\\overline{S}) = 0.6 *0.7 = 0.42\\)\n\\(P(\\overline{T} \\cap S) = P(\\overline{T} \\mid S) * P(S) = 0.8 *0.3 = 0.24\\)\n\\(P(\\overline{T} \\cap \\overline{S}) = P(\\overline{T} \\mid \\overline{S}) * P(\\overline{S}) = 0.7 *0.4 = 0.28\\)"
  },
  {
    "objectID": "content/01_journal/01_probability.html#given",
    "href": "content/01_journal/01_probability.html#given",
    "title": "Probability Theory",
    "section": "",
    "text": "## Solution\n\n\\(P(T \\cap S) = P(T \\mid S) * P(S) = 0.2 *0.3 = 0.06\\)\n\\(P(T \\cap \\overline{S}) = P(T \\mid \\overline{S}) * P(\\overline{S}) = 0.6 *0.7 = 0.42\\)\n\\(P(\\overline{T} \\cap S) = P(\\overline{T} \\mid S) * P(S) = 0.8 *0.3 = 0.24\\)\n\\(P(\\overline{T} \\cap \\overline{S}) = P(\\overline{T} \\mid \\overline{S}) * P(\\overline{S}) = 0.7 *0.4 = 0.28\\)"
  },
  {
    "objectID": "content/01_journal/01_probability.html#given-1",
    "href": "content/01_journal/01_probability.html#given-1",
    "title": "Probability Theory",
    "section": "Given",
    "text": "Given"
  },
  {
    "objectID": "content/01_journal/01_probability.html#solution-1",
    "href": "content/01_journal/01_probability.html#solution-1",
    "title": "Probability Theory",
    "section": "Solution",
    "text": "Solution\n\nPercentage of customers using all 3 devices:\n\\((Smartphone \\cap Tablet \\cap Computer) = 0.5\\%\\)\nPercentage of customers using at least 2 devices:\n\\((Smartphone \\cap Tablet \\cap Computer) + (Smartphone \\cap Tablet) +(Tablet \\cap Computer) + (Smartphone \\cap Computer)\\)\n\\(= 0.5\\% + 7.3\\% + 3.3\\% + 8.8\\% = 19.9\\%\\)\nPercentage of customers using only 1 device = Percentage of users - Percentage of users using at least 2 devices:\n\\(100\\% - 19.9\\% = 80.1\\%\\)"
  },
  {
    "objectID": "content/01_journal/01_probability.html#solution-2",
    "href": "content/01_journal/01_probability.html#solution-2",
    "title": "Probability Theory",
    "section": "Solution",
    "text": "Solution\n\n\\(P(\\overline{A} \\mid B) = \\frac{P(B \\mid \\overline{A}) * P(\\overline{A})}{P(B)} = \\frac{P(B \\mid \\overline{A}) * P(\\overline{A})}{P(B \\mid \\overline{A})*P(\\overline{A}) + P(B \\mid A) * P(A)} = \\frac{0.01*0.96}{(0.01*0.96)+(0.97*0.04)} = 0.198\\)\n\\(P(A \\mid B) = 1 - P(\\overline{A} \\mid B) = 1 - 0.192 = 0.802\\)\nOR\n\\(P(A \\mid B) = \\frac{P(B \\mid A) * P(A)}{P(B)} = \\frac{P(B \\mid A) * P(A)}{P(B \\mid A) * P(A) + P(B \\mid \\overline{A})*P(\\overline{A})} = \\frac{0.97*0.04}{(0.97*0.04)+(0.01*0.96)} = 0.802\\)"
  },
  {
    "objectID": "content/01_journal/02_statistics.html#load-library-and-data",
    "href": "content/01_journal/02_statistics.html#load-library-and-data",
    "title": "Statistical Concepts",
    "section": "",
    "text": "library(dplyr)\n\nrandom_vars &lt;- readRDS(\"~/Documents/TUHH/Casual Data Science for Business Analytics/Causal_Data_Science_Data/random_vars.rds\")\n\n\n\n\nageExpectedValue &lt;- summarise(random_vars, mean = mean(age))\nprint(ageExpectedValue)\n\n#&gt; # A tibble: 1 × 1\n#&gt;    mean\n#&gt;   &lt;dbl&gt;\n#&gt; 1  33.5\n\nageVariance &lt;- var(random_vars[,1])\nprint(ageVariance)\n\n#&gt;          age\n#&gt; age 340.6078\n\nageStandardDeviation &lt;- summarise(random_vars, sd = sd(age))\nprint(ageStandardDeviation)\n\n#&gt; # A tibble: 1 × 1\n#&gt;      sd\n#&gt;   &lt;dbl&gt;\n#&gt; 1  18.5\n\n\n\n\n\n\nincomeExpectedValue &lt;- summarise(random_vars, mean = mean(income))\nprint(incomeExpectedValue)\n\n#&gt; # A tibble: 1 × 1\n#&gt;    mean\n#&gt;   &lt;dbl&gt;\n#&gt; 1 3511.\n\nincomeVariance &lt;- var(random_vars[,2])\nprint(incomeVariance)\n\n#&gt;         income\n#&gt; income 8625646\n\nincomeStandardDeviation &lt;- summarise(random_vars, sd = sd(income))\nprint(incomeStandardDeviation)\n\n#&gt; # A tibble: 1 × 1\n#&gt;      sd\n#&gt;   &lt;dbl&gt;\n#&gt; 1 2937."
  },
  {
    "objectID": "content/01_journal/02_statistics.html#comparison",
    "href": "content/01_journal/02_statistics.html#comparison",
    "title": "Statistical Concepts",
    "section": "",
    "text": "Well I believe that there’s no sense to compare the standard deviations because simple the income and age aren’t directly related and hence we cannot say that if someone’s age is above the mean with a certain deviation that their income will increase by a certain deviation and vice versa"
  },
  {
    "objectID": "content/01_journal/02_statistics.html#covariance-and-correlation",
    "href": "content/01_journal/02_statistics.html#covariance-and-correlation",
    "title": "Statistical Concepts",
    "section": "",
    "text": "cov &lt;- cov(random_vars[,1],random_vars[,2])\nprint(cov)\n\n#&gt;       income\n#&gt; age 29700.15\n\ncor &lt;- cor(random_vars[,1],random_vars[,2])\nprint(cor)\n\n#&gt;        income\n#&gt; age 0.5479432"
  },
  {
    "objectID": "content/01_journal/02_statistics.html#conditional-expected-value",
    "href": "content/01_journal/02_statistics.html#conditional-expected-value",
    "title": "Statistical Concepts",
    "section": "",
    "text": "\\(E[income \\mid age &gt;= 18]\\)\n\n\ne1 &lt;- summarise(random_vars[random_vars$age&gt;=18,], mean = mean(income))\nprint(e1)\n\n#&gt; # A tibble: 1 × 1\n#&gt;    mean\n#&gt;   &lt;dbl&gt;\n#&gt; 1 4464.\n\n\n\n\\(E[income \\mid age \\in [18,65)]\\)\n\n\ne2 &lt;- summarise(random_vars[random_vars$age &gt;= 18 & random_vars$age&lt;65,], mean = mean(income))\nprint(e2)\n\n#&gt; # A tibble: 1 × 1\n#&gt;    mean\n#&gt;   &lt;dbl&gt;\n#&gt; 1 4686.\n\n\n\n\\(E[income \\mid age &gt;= 65]\\)\n\n\ne3 &lt;- summarise(random_vars[random_vars$age &gt;= 65,], mean = mean(income))\nprint(e3)\n\n#&gt; # A tibble: 1 × 1\n#&gt;    mean\n#&gt;   &lt;dbl&gt;\n#&gt; 1 1777."
  },
  {
    "objectID": "content/01_journal/02_statistics.html#correlation-vs-covariance",
    "href": "content/01_journal/02_statistics.html#correlation-vs-covariance",
    "title": "Statistical Concepts",
    "section": "",
    "text": "Correlation is easier to interpret because it is bounded with the magnitude showing us how strong or how weak is the relationship and the sign shows us the direction. Meanwhile the covariance is value that doesn’t really tell us much, in terms of the number provided."
  },
  {
    "objectID": "content/01_journal/03_regression.html#import-the-data-set-and-check-dimensions",
    "href": "content/01_journal/03_regression.html#import-the-data-set-and-check-dimensions",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "library(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.3     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ncar_prices &lt;- readRDS(\"~/Documents/TUHH/Casual Data Science for Business Analytics/Causal_Data_Science_Data/car_prices.rds\")\n\ndim(car_prices)\n\n#&gt; [1] 181  22\n\n\nThe dimensions of the data set is 188 rows and 22 coloumns"
  },
  {
    "objectID": "content/01_journal/03_regression.html#data-glimpse-and-lookup",
    "href": "content/01_journal/03_regression.html#data-glimpse-and-lookup",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "glimpse(car_prices)\n\n#&gt; Rows: 181\n#&gt; Columns: 22\n#&gt; $ aspiration       &lt;chr&gt; \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std…\n#&gt; $ doornumber       &lt;chr&gt; \"two\", \"two\", \"two\", \"four\", \"four\", \"two\", \"four\", \"…\n#&gt; $ carbody          &lt;chr&gt; \"convertible\", \"convertible\", \"hatchback\", \"sedan\", \"…\n#&gt; $ drivewheel       &lt;chr&gt; \"rwd\", \"rwd\", \"rwd\", \"fwd\", \"4wd\", \"fwd\", \"fwd\", \"fwd…\n#&gt; $ enginelocation   &lt;chr&gt; \"front\", \"front\", \"front\", \"front\", \"front\", \"front\",…\n#&gt; $ wheelbase        &lt;dbl&gt; 88.6, 88.6, 94.5, 99.8, 99.4, 99.8, 105.8, 105.8, 105…\n#&gt; $ carlength        &lt;dbl&gt; 168.8, 168.8, 171.2, 176.6, 176.6, 177.3, 192.7, 192.…\n#&gt; $ carwidth         &lt;dbl&gt; 64.1, 64.1, 65.5, 66.2, 66.4, 66.3, 71.4, 71.4, 71.4,…\n#&gt; $ carheight        &lt;dbl&gt; 48.8, 48.8, 52.4, 54.3, 54.3, 53.1, 55.7, 55.7, 55.9,…\n#&gt; $ curbweight       &lt;dbl&gt; 2548, 2548, 2823, 2337, 2824, 2507, 2844, 2954, 3086,…\n#&gt; $ enginetype       &lt;chr&gt; \"dohc\", \"dohc\", \"ohcv\", \"ohc\", \"ohc\", \"ohc\", \"ohc\", \"…\n#&gt; $ cylindernumber   &lt;chr&gt; \"four\", \"four\", \"six\", \"four\", \"five\", \"five\", \"five\"…\n#&gt; $ enginesize       &lt;dbl&gt; 130, 130, 152, 109, 136, 136, 136, 136, 131, 131, 108…\n#&gt; $ fuelsystem       &lt;chr&gt; \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi…\n#&gt; $ boreratio        &lt;dbl&gt; 3.47, 3.47, 2.68, 3.19, 3.19, 3.19, 3.19, 3.19, 3.13,…\n#&gt; $ stroke           &lt;dbl&gt; 2.68, 2.68, 3.47, 3.40, 3.40, 3.40, 3.40, 3.40, 3.40,…\n#&gt; $ compressionratio &lt;dbl&gt; 9.00, 9.00, 9.00, 10.00, 8.00, 8.50, 8.50, 8.50, 8.30…\n#&gt; $ horsepower       &lt;dbl&gt; 111, 111, 154, 102, 115, 110, 110, 110, 140, 160, 101…\n#&gt; $ peakrpm          &lt;dbl&gt; 5000, 5000, 5000, 5500, 5500, 5500, 5500, 5500, 5500,…\n#&gt; $ citympg          &lt;dbl&gt; 21, 21, 19, 24, 18, 19, 19, 19, 17, 16, 23, 23, 21, 2…\n#&gt; $ highwaympg       &lt;dbl&gt; 27, 27, 26, 30, 22, 25, 25, 25, 20, 22, 29, 29, 28, 2…\n#&gt; $ price            &lt;dbl&gt; 13495.00, 16500.00, 16500.00, 13950.00, 17450.00, 152…\n\n\nHere I used glimpse instead of head to help me see all the coloums and their data types. Strings are recorded as chars and numbers are double allowing decimal values."
  },
  {
    "objectID": "content/01_journal/03_regression.html#linear-regression",
    "href": "content/01_journal/03_regression.html#linear-regression",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "lm_all &lt;- lm(price ~ ., data = car_prices)\nsummary(lm_all)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ ., data = car_prices)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -5662  -1120      0    798   9040 \n#&gt; \n#&gt; Coefficients:\n#&gt;                        Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)          -36269.965  15460.866  -2.346 0.020354 *  \n#&gt; aspirationturbo        1846.206   1041.391   1.773 0.078386 .  \n#&gt; doornumbertwo           242.523    571.929   0.424 0.672172    \n#&gt; carbodyhardtop        -3691.743   1424.825  -2.591 0.010561 *  \n#&gt; carbodyhatchback      -3344.335   1238.359  -2.701 0.007757 ** \n#&gt; carbodysedan          -2292.820   1356.014  -1.691 0.093043 .  \n#&gt; carbodywagon          -3427.921   1490.285  -2.300 0.022885 *  \n#&gt; drivewheelfwd          -504.564   1076.623  -0.469 0.640030    \n#&gt; drivewheelrwd           -15.446   1268.070  -0.012 0.990299    \n#&gt; enginelocationrear     6643.492   2572.275   2.583 0.010806 *  \n#&gt; wheelbase               -30.197     92.776  -0.325 0.745294    \n#&gt; carlength               -29.740     51.672  -0.576 0.565824    \n#&gt; carwidth                731.819    244.533   2.993 0.003258 ** \n#&gt; carheight               123.195    134.607   0.915 0.361617    \n#&gt; curbweight                2.612      1.781   1.467 0.144706    \n#&gt; enginetypedohcv       -8541.957   4749.685  -1.798 0.074219 .  \n#&gt; enginetypel             978.748   1786.384   0.548 0.584619    \n#&gt; enginetypeohc          3345.252    933.001   3.585 0.000461 ***\n#&gt; enginetypeohcf          972.919   1625.631   0.598 0.550462    \n#&gt; enginetypeohcv        -6222.322   1236.415  -5.033 1.43e-06 ***\n#&gt; cylindernumberfive   -11724.540   3019.192  -3.883 0.000157 ***\n#&gt; cylindernumberfour   -11549.326   3177.177  -3.635 0.000387 ***\n#&gt; cylindernumbersix     -7151.398   2247.230  -3.182 0.001793 ** \n#&gt; cylindernumberthree   -4318.929   4688.833  -0.921 0.358545    \n#&gt; cylindernumbertwelve -11122.209   4196.494  -2.650 0.008946 ** \n#&gt; enginesize              125.934     26.541   4.745 5.00e-06 ***\n#&gt; fuelsystem2bbl          177.136    883.615   0.200 0.841400    \n#&gt; fuelsystemmfi         -3041.018   2576.996  -1.180 0.239934    \n#&gt; fuelsystemmpfi          359.278   1001.529   0.359 0.720326    \n#&gt; fuelsystemspdi        -2543.890   1363.546  -1.866 0.064140 .  \n#&gt; fuelsystemspfi          514.766   2499.229   0.206 0.837107    \n#&gt; boreratio             -1306.740   1642.221  -0.796 0.427516    \n#&gt; stroke                -4527.137    922.732  -4.906 2.49e-06 ***\n#&gt; compressionratio       -737.901    555.960  -1.327 0.186539    \n#&gt; horsepower               10.293     22.709   0.453 0.651035    \n#&gt; peakrpm                   2.526      0.634   3.983 0.000108 ***\n#&gt; citympg                 -90.352    166.647  -0.542 0.588538    \n#&gt; highwaympg              154.858    167.148   0.926 0.355761    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2189 on 143 degrees of freedom\n#&gt; Multiple R-squared:  0.9415, Adjusted R-squared:  0.9264 \n#&gt; F-statistic: 62.21 on 37 and 143 DF,  p-value: &lt; 2.2e-16\n\n\nFrom the summary we could see that the main factors relevant are:\n\nenginetype (mostly)\neniginesize\nstroke\npeakrpm"
  },
  {
    "objectID": "content/01_journal/03_regression.html#choosing-data-regressor",
    "href": "content/01_journal/03_regression.html#choosing-data-regressor",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "lm_top &lt;- lm(price ~ enginetype +\n                     enginesize +\n                     stroke +\n                     peakrpm,\n                     data = car_prices)\nsummary(lm_top)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ enginetype + enginesize + stroke + peakrpm, \n#&gt;     data = car_prices)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -14872.1  -1453.3   -410.3   1563.4  10058.4 \n#&gt; \n#&gt; Coefficients:\n#&gt;                   Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)     -1.618e+04  4.220e+03  -3.834 0.000177 ***\n#&gt; enginetypedohcv  1.280e+03  3.135e+03   0.408 0.683638    \n#&gt; enginetypel      3.842e+03  1.480e+03   2.596 0.010256 *  \n#&gt; enginetypeohc    2.550e+03  9.641e+02   2.645 0.008925 ** \n#&gt; enginetypeohcf   6.534e+02  1.283e+03   0.509 0.611213    \n#&gt; enginetypeohcv  -4.967e+03  1.284e+03  -3.870 0.000154 ***\n#&gt; enginesize       2.145e+02  7.630e+00  28.106  &lt; 2e-16 ***\n#&gt; stroke          -5.398e+03  9.088e+02  -5.939 1.55e-08 ***\n#&gt; peakrpm          3.400e+00  5.522e-01   6.158 5.05e-09 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2971 on 172 degrees of freedom\n#&gt; Multiple R-squared:  0.8704, Adjusted R-squared:  0.8644 \n#&gt; F-statistic: 144.4 on 8 and 172 DF,  p-value: &lt; 2.2e-16\n\n\nAs we can see the smallest p-value is the one for engine size and hence im picking it to be my regressor.\n\nmax(car_prices$enginesize)\n\n#&gt; [1] 326\n\nmin(car_prices$enginesize)\n\n#&gt; [1] 61\n\n\nThe regressor I chose is of type double and its values in this data set ranges from 61 to 326.\nIt has a big effect on the price since the p value is very small it denies the null hypothesis and tells that that there is a strong corellation between both factors.\n\nlm_summarised &lt;- lm(price ~ enginesize, data = car_prices)\nconfint(lm_summarised, level = 0.95)\n\n#&gt;                   2.5 %     97.5 %\n#&gt; (Intercept) -10346.0474 -6898.5441\n#&gt; enginesize     157.1932   182.9353\n\n\nThe estimate is completely positive hence the effect is statistically significant."
  },
  {
    "objectID": "content/01_journal/03_regression.html#adding-the-variable",
    "href": "content/01_journal/03_regression.html#adding-the-variable",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "new_car_prices &lt;- mutate(car_prices, seat_heating = TRUE)\nlm_heat &lt;- lm(price ~ ., data = new_car_prices)\nsummary(lm_heat)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ ., data = new_car_prices)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -5662  -1120      0    798   9040 \n#&gt; \n#&gt; Coefficients: (1 not defined because of singularities)\n#&gt;                        Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)          -36269.965  15460.866  -2.346 0.020354 *  \n#&gt; aspirationturbo        1846.206   1041.391   1.773 0.078386 .  \n#&gt; doornumbertwo           242.523    571.929   0.424 0.672172    \n#&gt; carbodyhardtop        -3691.743   1424.825  -2.591 0.010561 *  \n#&gt; carbodyhatchback      -3344.335   1238.359  -2.701 0.007757 ** \n#&gt; carbodysedan          -2292.820   1356.014  -1.691 0.093043 .  \n#&gt; carbodywagon          -3427.921   1490.285  -2.300 0.022885 *  \n#&gt; drivewheelfwd          -504.564   1076.623  -0.469 0.640030    \n#&gt; drivewheelrwd           -15.446   1268.070  -0.012 0.990299    \n#&gt; enginelocationrear     6643.492   2572.275   2.583 0.010806 *  \n#&gt; wheelbase               -30.197     92.776  -0.325 0.745294    \n#&gt; carlength               -29.740     51.672  -0.576 0.565824    \n#&gt; carwidth                731.819    244.533   2.993 0.003258 ** \n#&gt; carheight               123.195    134.607   0.915 0.361617    \n#&gt; curbweight                2.612      1.781   1.467 0.144706    \n#&gt; enginetypedohcv       -8541.957   4749.685  -1.798 0.074219 .  \n#&gt; enginetypel             978.748   1786.384   0.548 0.584619    \n#&gt; enginetypeohc          3345.252    933.001   3.585 0.000461 ***\n#&gt; enginetypeohcf          972.919   1625.631   0.598 0.550462    \n#&gt; enginetypeohcv        -6222.322   1236.415  -5.033 1.43e-06 ***\n#&gt; cylindernumberfive   -11724.540   3019.192  -3.883 0.000157 ***\n#&gt; cylindernumberfour   -11549.326   3177.177  -3.635 0.000387 ***\n#&gt; cylindernumbersix     -7151.398   2247.230  -3.182 0.001793 ** \n#&gt; cylindernumberthree   -4318.929   4688.833  -0.921 0.358545    \n#&gt; cylindernumbertwelve -11122.209   4196.494  -2.650 0.008946 ** \n#&gt; enginesize              125.934     26.541   4.745 5.00e-06 ***\n#&gt; fuelsystem2bbl          177.136    883.615   0.200 0.841400    \n#&gt; fuelsystemmfi         -3041.018   2576.996  -1.180 0.239934    \n#&gt; fuelsystemmpfi          359.278   1001.529   0.359 0.720326    \n#&gt; fuelsystemspdi        -2543.890   1363.546  -1.866 0.064140 .  \n#&gt; fuelsystemspfi          514.766   2499.229   0.206 0.837107    \n#&gt; boreratio             -1306.740   1642.221  -0.796 0.427516    \n#&gt; stroke                -4527.137    922.732  -4.906 2.49e-06 ***\n#&gt; compressionratio       -737.901    555.960  -1.327 0.186539    \n#&gt; horsepower               10.293     22.709   0.453 0.651035    \n#&gt; peakrpm                   2.526      0.634   3.983 0.000108 ***\n#&gt; citympg                 -90.352    166.647  -0.542 0.588538    \n#&gt; highwaympg              154.858    167.148   0.926 0.355761    \n#&gt; seat_heatingTRUE             NA         NA      NA       NA    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2189 on 143 degrees of freedom\n#&gt; Multiple R-squared:  0.9415, Adjusted R-squared:  0.9264 \n#&gt; F-statistic: 62.21 on 37 and 143 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/05_dag.html#parking-spots-example",
    "href": "content/01_journal/05_dag.html#parking-spots-example",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "In the parking spots example we had the parking spots as treatment variables, the stores as observation units and the sales as an outcome also taking into consideration the location as a confounder.\n\n# Load packages\nlibrary(dagitty)\nlibrary(ggdag)\n\n#&gt; \n#&gt; Attaching package: 'ggdag'\n\n\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n\nlibrary(ggplot2)\n\n#Create DAG model\nparking_model &lt;- dagify(\n  S ~ L,\n  A ~ P,\n  B ~ P,\n  C ~ P,\n  D ~ P,\n  S ~ A,\n  S ~ B,\n  S ~ C,\n  S ~ D,\n  coords = list(x = c(L = 7,P = 1,A = 3,B = 3,C = 3,D = 3,S = 7),\n                y = c(L = 1,P = 0,A = 1,B = 0.5,C = -0.5,D = -1,S = 0)),\n  labels = list(L=\"location\",\n                P = \"parking spots\",\n                A = \"Store A\",\n                B = \"Store B\",\n                C = \"Store C\",\n                D = \"Store D\",\n                S = \"Sales\")\n)\n\n#Plot DAG model\nggdag(parking_model) +\n  theme_dag()+\n  geom_dag_node(color = \"red\")+\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"black\") +\n  geom_dag_label_repel(aes(label = label))"
  },
  {
    "objectID": "content/01_journal/05_dag.html#customer-satisfaction",
    "href": "content/01_journal/05_dag.html#customer-satisfaction",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "library(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.3     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#&gt; ✔ lubridate 1.9.3     ✔ tibble    3.2.1\n#&gt; ✔ purrr     1.0.2     ✔ tidyr     1.3.0\n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks ggdag::filter(), stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ncustomer_sat &lt;- readRDS(\"~/Documents/TUHH/Casual Data Science for Business Analytics/Causal_Data_Science_Data/customer_sat.rds\")\nhead(customer_sat)\n\n\n\n  \n\n\nlm_first &lt;- lm(satisfaction ~ follow_ups, data = customer_sat)\nsummary(lm_first)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups, data = customer_sat)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -12.412  -5.257   1.733   4.506  12.588 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  78.8860     4.2717  18.467 1.04e-10 ***\n#&gt; follow_ups   -3.3093     0.6618  -5.001 0.000243 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 7.923 on 13 degrees of freedom\n#&gt; Multiple R-squared:  0.658,  Adjusted R-squared:  0.6316 \n#&gt; F-statistic: 25.01 on 1 and 13 DF,  p-value: 0.0002427\n\nlm_second &lt;- lm(satisfaction ~ follow_ups + subscription, data = customer_sat)\nsummary(lm_second)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups + subscription, data = customer_sat)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -4.3222 -2.1972  0.3167  2.2667  3.9944 \n#&gt; \n#&gt; Coefficients:\n#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)           26.7667     6.6804   4.007  0.00206 ** \n#&gt; follow_ups             2.1944     0.7795   2.815  0.01682 *  \n#&gt; subscriptionPremium   44.7222     5.6213   7.956 6.88e-06 ***\n#&gt; subscriptionPremium+  18.0722     2.1659   8.344 4.37e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.958 on 11 degrees of freedom\n#&gt; Multiple R-squared:  0.9597, Adjusted R-squared:  0.9487 \n#&gt; F-statistic: 87.21 on 3 and 11 DF,  p-value: 5.956e-08\n\n\n\n\n\nOne of the possible reasons for having a negative estimate in the first regression and then a positive one in the second could be due to the nature of the follow-up calls. So when we speak about follow-ups in general its usually a problem and at lower subscription levels the solution could be to upgrade their subscription which is more expensive and hence the customer isn’t satisfied with the outcome, it could also be a bug or a complain which applies to all subscription levels but when you factor in the subscription level the nature of the calls could be about an upgrade or an inquiry about a feature that gets resolved and the customer is satisfied.\nAnother explanation that appears later when plotting the data is that when we don’t factor the subscription levels it shows that overall the more the follow-ups the less the satisfaction but when you factor the subscription you find that every subscription level has its own range of satisfaction which when isolated shows us an increase in satisfaction as the increase of follow-ups.\n\n\n\n\nno_sub_sat &lt;- ggplot(customer_sat, aes(x = follow_ups, y = satisfaction)) +\n  geom_point(alpha = .8) +\n  stat_smooth(method = \"lm\", se = F)\n\nsub_sat &lt;- ggplot(customer_sat, aes(x = follow_ups, y = satisfaction, color = subscription)) +\n  geom_point(alpha = .8) +\n  stat_smooth(method = \"lm\", se = F)\n\nno_sub_sat\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nsub_sat\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "content/01_journal/06_rct.html#checking-whether-covariates-are-balanced-throughout-the-groups.",
    "href": "content/01_journal/06_rct.html#checking-whether-covariates-are-balanced-throughout-the-groups.",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "library(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.3     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nabtest &lt;- readRDS(\"~/Documents/TUHH/Casual Data Science for Business Analytics/Causal_Data_Science_Data/abtest_online.rds\")\n\ncompare &lt;- \n  ggplot(abtest, \n         aes(x = chatbot, \n             y = purchase_amount, \n             color = as.factor(chatbot))) +\n  stat_summary(geom = \"errorbar\", \n               width = .5,\n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"Purchase Amount\", title = \"Difference in Purchase Amount\")\n\ncompare\n\n\n\n\n\n\n\n\nAs we can see in the plot the covariates are not balanced."
  },
  {
    "objectID": "content/01_journal/06_rct.html#chatbot-effect-on-sales",
    "href": "content/01_journal/06_rct.html#chatbot-effect-on-sales",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "lm_sales &lt;- lm(purchase_amount ~ chatbot, data = abtest)\nsummary(lm_sales)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot, data = abtest)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -16.702 -14.478  -9.626  13.922  64.648 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  16.7017     0.8374  19.944  &lt; 2e-16 ***\n#&gt; chatbotTRUE  -7.0756     1.1796  -5.998 2.79e-09 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.65 on 998 degrees of freedom\n#&gt; Multiple R-squared:  0.0348, Adjusted R-squared:  0.03383 \n#&gt; F-statistic: 35.98 on 1 and 998 DF,  p-value: 2.787e-09\n\n\nThe coefficient is a negative number telling us that the chatbots are affecting the sales in a negative way decreasing the sales and not increasing them."
  },
  {
    "objectID": "content/01_journal/06_rct.html#cate-taking-into-consideration-mobile-users",
    "href": "content/01_journal/06_rct.html#cate-taking-into-consideration-mobile-users",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "lm_sales_plus_mob &lt;- lm(purchase_amount ~ chatbot*mobile_device, data = abtest)\nsummary(lm_sales_plus_mob)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot * mobile_device, data = abtest)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -16.98 -14.54  -9.95  14.13  65.24 \n#&gt; \n#&gt; Coefficients:\n#&gt;                               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)                    16.9797     1.0152  16.725   &lt;2e-16 ***\n#&gt; chatbotTRUE                    -7.0301     1.4284  -4.922    1e-06 ***\n#&gt; mobile_deviceTRUE              -0.8727     1.7987  -0.485    0.628    \n#&gt; chatbotTRUE:mobile_deviceTRUE  -0.1526     2.5369  -0.060    0.952    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.66 on 996 degrees of freedom\n#&gt; Multiple R-squared:  0.03534,    Adjusted R-squared:  0.03244 \n#&gt; F-statistic: 12.16 on 3 and 996 DF,  p-value: 8.034e-08"
  },
  {
    "objectID": "content/01_journal/06_rct.html#logistic-regression-for-purchase",
    "href": "content/01_journal/06_rct.html#logistic-regression-for-purchase",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "glm_sales &lt;- glm(purchase ~ chatbot, family = binomial(link = 'logit'), abtest)\nsummary(glm_sales)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = purchase ~ chatbot, family = binomial(link = \"logit\"), \n#&gt;     data = abtest)\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept) -0.01613    0.08981  -0.180    0.857    \n#&gt; chatbotTRUE -0.98939    0.13484  -7.337 2.18e-13 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 1329.1  on 999  degrees of freedom\n#&gt; Residual deviance: 1273.3  on 998  degrees of freedom\n#&gt; AIC: 1277.3\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\n\nIn logistic regression we don’t just look at the coefficient but its exponent and that tells us the “odds ratio” which is the ratio of the probability of an event happening divided by the probability of the non event.\nSince the coefficient here is a negative number hence the odds ratio is less than one and that tells us that the event will most likely not happen.\nIn our case the event is the purchase and so the regression tells us that having the chatbot means that the probability of the customer making a purchase is less than that when the customer doesn’t have a chat bot. This means that the chat bot is negatively affecting the number of purchases and hence decreasing the sales."
  },
  {
    "objectID": "content/01_journal/07_matching.html#dag-model",
    "href": "content/01_journal/07_matching.html#dag-model",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "library(tidyverse)\nlibrary(patchwork)\nlibrary(dagitty)\nlibrary(ggdag)\nlibrary(ggplot2)\nlibrary(MatchIt)\n\nstore &lt;- readRDS(\"~/Documents/TUHH/Casual Data Science for Business Analytics/Causal_Data_Science_Data/membership.rds\")\n\nstore\n\n\n\n  \n\n\nstore_model &lt;- dagify(\n  M ~ A,\n  M ~ S,\n  M ~ P,\n  R ~ A,\n  R ~ S,\n  R ~ P,\n  coords = list(x = c(A = 1,S = 4,P = 7,M = 1,R = 7),\n                y = c(A = 2,S = 2,P = 2,M = 0,R = 0)),\n  labels = list(A = \"Age\",\n                P = \"Previousverage Purchase\",\n                S = \"Sex\",\n                M = \"Membership\",\n                R = \"Revenue\")\n)\n\n#Plot DAG model\nggdag(store_model) +\n  theme_dag()+\n  geom_dag_node(color = \"red\")+\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"black\") +\n  geom_dag_label_repel(aes(label = label))"
  },
  {
    "objectID": "content/01_journal/07_matching.html#naive-estimate-of-the-average-treatment-effect",
    "href": "content/01_journal/07_matching.html#naive-estimate-of-the-average-treatment-effect",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "naive_lm &lt;- lm(avg_purch ~ card, data = store)\nsummary(naive_lm)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = store)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -101.515  -20.684   -0.199   20.424  120.166 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  65.9397     0.3965  166.29   &lt;2e-16 ***\n#&gt; card         25.2195     0.6095   41.38   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.11 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.1462, Adjusted R-squared:  0.1461 \n#&gt; F-statistic:  1712 on 1 and 9998 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/07_matching.html#coarsened-exact-matching",
    "href": "content/01_journal/07_matching.html#coarsened-exact-matching",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "cem &lt;- matchit(card  ~ avg_purch + age + sex + pre_avg_purch,\n               data = store, \n               method = 'cem', \n               estimand = 'ATE')\n\ndf_cem &lt;- match.data(cem)\n\ncem_lm &lt;- lm(avg_purch ~ card, data = df_cem, weights = weights)\nsummary(cem_lm)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_cem, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -162.422  -19.058    0.153   18.795  147.646 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  75.9624     0.4051 187.526   &lt;2e-16 ***\n#&gt; card          0.9485     0.6212   1.527    0.127    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 28.34 on 8515 degrees of freedom\n#&gt; Multiple R-squared:  0.0002737,  Adjusted R-squared:  0.0001563 \n#&gt; F-statistic: 2.332 on 1 and 8515 DF,  p-value: 0.1268"
  },
  {
    "objectID": "content/01_journal/07_matching.html#nearest-neighbor-matching",
    "href": "content/01_journal/07_matching.html#nearest-neighbor-matching",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "nn &lt;- matchit(card  ~ avg_purch + age + sex + pre_avg_purch,\n              data = store,\n              method = \"nearest\", # changed\n              distance = \"mahalanobis\", # changed\n              replace = T)\n\ndf_nn &lt;- match.data(nn)\n\nnn_lm &lt;- lm(avg_purch ~ card, data = df_nn, weights = weights)\nsummary(nn_lm)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_nn, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -101.515  -22.370   -3.279   16.475  178.481 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  90.2268     0.6438 140.155   &lt;2e-16 ***\n#&gt; card          0.9324     0.7939   1.174     0.24    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.22 on 6434 degrees of freedom\n#&gt; Multiple R-squared:  0.0002143,  Adjusted R-squared:  5.895e-05 \n#&gt; F-statistic: 1.379 on 1 and 6434 DF,  p-value: 0.2403"
  },
  {
    "objectID": "content/01_journal/07_matching.html#inverse-probability-weighting",
    "href": "content/01_journal/07_matching.html#inverse-probability-weighting",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "model_prop &lt;- glm(card  ~ avg_purch + age + sex + pre_avg_purch,\n                  data = store,\n                  family = binomial(link = \"logit\"))\n\n# Add propensities to table\ndf_aug &lt;- store %&gt;% mutate(propensity = predict(model_prop, type = \"response\"))\n\n# Extend data by IPW scores\ndf_ipw &lt;- df_aug %&gt;% mutate(\n  ipw = (card/propensity) + ((1-card) / (1-propensity)))\n\n#Estimation\nipw_lm &lt;- lm(avg_purch ~ card,\n                data = df_ipw, \n                weights = ipw)\nsummary(ipw_lm)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_ipw, weights = ipw)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -525.64  -28.55   -0.11   30.04  441.02 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  76.5744     0.4664 164.189   &lt;2e-16 ***\n#&gt; card         -0.6361     0.6577  -0.967    0.333    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 46.6 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  9.356e-05,  Adjusted R-squared:  -6.45e-06 \n#&gt; F-statistic: 0.9355 on 1 and 9998 DF,  p-value: 0.3335"
  },
  {
    "objectID": "content/01_journal/08_did.html#load-data",
    "href": "content/01_journal/08_did.html#load-data",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "library(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.3     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nhospitals &lt;- readRDS(\"~/Documents/TUHH/Casual Data Science for Business Analytics/Causal_Data_Science_Data/hospdd.rds\")"
  },
  {
    "objectID": "content/01_journal/08_did.html#manual-computing",
    "href": "content/01_journal/08_did.html#manual-computing",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "# Step 1: Difference between treatment and control group BEFORE treatment\nbefore_control &lt;- hospitals %&gt;%\n  filter(hospital &gt;= 19 & hospital&lt;= 46, procedure == 0) %&gt;% \n  pull(satis)\nbefore_treatment &lt;- hospitals %&gt;%\n  filter(hospital &gt;= 1 & hospital&lt;=18, procedure == 0) %&gt;% \n  pull(satis)\n\ndiff_before &lt;- before_treatment - before_control\n\n#&gt; Warning in before_treatment - before_control: longer object length is not a\n#&gt; multiple of shorter object length\n\n# Step 2: Difference between treatment and control group AFTER treatment\nafter_control &lt;- rep(0, dim(hospitals)[1])\nafter_treatment &lt;- hospitals %&gt;%\n  filter(hospital &gt;= 1 & hospital&lt;=18, procedure == 1) %&gt;% \n  pull(satis)\n\ndiff_after &lt;- after_treatment - after_control\n\n#&gt; Warning in after_treatment - after_control: longer object length is not a\n#&gt; multiple of shorter object length\n\ndiff_diff &lt;- diff_after - diff_before\n\n#&gt; Warning in diff_after - diff_before: longer object length is not a multiple of\n#&gt; shorter object length\n\n# sprintf(\"Estimate: %.2f\", diff_diff)\n# didn't print it cause its very wrong"
  },
  {
    "objectID": "content/01_journal/08_did.html#linear-regrression",
    "href": "content/01_journal/08_did.html#linear-regrression",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "lm_1 &lt;- lm(satis ~ month + hospital, data = hospitals)\nsummary(lm_1)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satis ~ month + hospital, data = hospitals)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -3.3831 -0.6724 -0.0838  0.5778  5.7881 \n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  3.7597212  0.0308145  122.01   &lt;2e-16 ***\n#&gt; month        0.0720728  0.0055957   12.88   &lt;2e-16 ***\n#&gt; hospital    -0.0175982  0.0008732  -20.16   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 1.017 on 7365 degrees of freedom\n#&gt; Multiple R-squared:  0.07208,    Adjusted R-squared:  0.07183 \n#&gt; F-statistic: 286.1 on 2 and 7365 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\nlm_2 &lt;- lm(satis ~ as.factor(month) + as.factor(hospital), data = hospitals)\nsummary(lm_2)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satis ~ as.factor(month) + as.factor(hospital), \n#&gt;     data = hospitals)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -3.4357 -0.4930 -0.0120  0.4755  4.5398 \n#&gt; \n#&gt; Coefficients:\n#&gt;                        Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)            3.419332   0.057597  59.367  &lt; 2e-16 ***\n#&gt; as.factor(month)2     -0.009608   0.030411  -0.316 0.752069    \n#&gt; as.factor(month)3      0.021969   0.030411   0.722 0.470083    \n#&gt; as.factor(month)4      0.349354   0.030411  11.488  &lt; 2e-16 ***\n#&gt; as.factor(month)5      0.343235   0.030411  11.286  &lt; 2e-16 ***\n#&gt; as.factor(month)6      0.348800   0.030411  11.469  &lt; 2e-16 ***\n#&gt; as.factor(month)7      0.341444   0.030411  11.228  &lt; 2e-16 ***\n#&gt; as.factor(hospital)2   0.408566   0.080413   5.081 3.85e-07 ***\n#&gt; as.factor(hospital)3   0.533625   0.082596   6.461 1.11e-10 ***\n#&gt; as.factor(hospital)4   0.227510   0.076977   2.956 0.003131 ** \n#&gt; as.factor(hospital)5  -0.145353   0.076977  -1.888 0.059030 .  \n#&gt; as.factor(hospital)6   0.447863   0.076977   5.818 6.20e-09 ***\n#&gt; as.factor(hospital)7   1.404416   0.074390  18.879  &lt; 2e-16 ***\n#&gt; as.factor(hospital)8   0.071876   0.079452   0.905 0.365685    \n#&gt; as.factor(hospital)9  -1.518515   0.081457 -18.642  &lt; 2e-16 ***\n#&gt; as.factor(hospital)10  1.682845   0.080413  20.927  &lt; 2e-16 ***\n#&gt; as.factor(hospital)11  0.220965   0.079452   2.781 0.005431 ** \n#&gt; as.factor(hospital)12 -0.095303   0.081457  -1.170 0.242047    \n#&gt; as.factor(hospital)13  0.495593   0.078564   6.308 2.99e-10 ***\n#&gt; as.factor(hospital)14  0.233043   0.082596   2.821 0.004793 ** \n#&gt; as.factor(hospital)15 -0.144494   0.082596  -1.749 0.080263 .  \n#&gt; as.factor(hospital)16  1.414268   0.080413  17.588  &lt; 2e-16 ***\n#&gt; as.factor(hospital)17  0.423543   0.083843   5.052 4.49e-07 ***\n#&gt; as.factor(hospital)18  0.153276   0.097668   1.569 0.116609    \n#&gt; as.factor(hospital)19 -1.169296   0.082596 -14.157  &lt; 2e-16 ***\n#&gt; as.factor(hospital)20 -0.376607   0.080413  -4.683 2.87e-06 ***\n#&gt; as.factor(hospital)21  0.770343   0.085215   9.040  &lt; 2e-16 ***\n#&gt; as.factor(hospital)22  0.375321   0.083843   4.476 7.70e-06 ***\n#&gt; as.factor(hospital)23  0.277726   0.082596   3.362 0.000776 ***\n#&gt; as.factor(hospital)24 -0.732120   0.088421  -8.280  &lt; 2e-16 ***\n#&gt; as.factor(hospital)25  0.222480   0.094875   2.345 0.019055 *  \n#&gt; as.factor(hospital)26 -0.209747   0.080413  -2.608 0.009116 ** \n#&gt; as.factor(hospital)27 -0.822648   0.077742 -10.582  &lt; 2e-16 ***\n#&gt; as.factor(hospital)28  0.288001   0.085215   3.380 0.000729 ***\n#&gt; as.factor(hospital)29 -0.175443   0.081457  -2.154 0.031288 *  \n#&gt; as.factor(hospital)30 -0.591916   0.097668  -6.060 1.42e-09 ***\n#&gt; as.factor(hospital)31  0.088091   0.080413   1.095 0.273344    \n#&gt; as.factor(hospital)32 -0.747340   0.081457  -9.175  &lt; 2e-16 ***\n#&gt; as.factor(hospital)33 -0.877969   0.080413 -10.918  &lt; 2e-16 ***\n#&gt; as.factor(hospital)34 -0.424406   0.075599  -5.614 2.05e-08 ***\n#&gt; as.factor(hospital)35 -0.069883   0.077742  -0.899 0.368729    \n#&gt; as.factor(hospital)36  1.714149   0.078564  21.818  &lt; 2e-16 ***\n#&gt; as.factor(hospital)37 -0.283590   0.094875  -2.989 0.002807 ** \n#&gt; as.factor(hospital)38 -0.510800   0.079452  -6.429 1.36e-10 ***\n#&gt; as.factor(hospital)39 -0.447491   0.083843  -5.337 9.72e-08 ***\n#&gt; as.factor(hospital)40  0.697539   0.079452   8.779  &lt; 2e-16 ***\n#&gt; as.factor(hospital)41 -0.573729   0.077742  -7.380 1.76e-13 ***\n#&gt; as.factor(hospital)42  0.457143   0.086733   5.271 1.40e-07 ***\n#&gt; as.factor(hospital)43 -1.196426   0.082596 -14.485  &lt; 2e-16 ***\n#&gt; as.factor(hospital)44 -0.389582   0.092446  -4.214 2.54e-05 ***\n#&gt; as.factor(hospital)45 -0.637743   0.077742  -8.203 2.74e-16 ***\n#&gt; as.factor(hospital)46 -0.345502   0.083843  -4.121 3.82e-05 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.7536 on 7316 degrees of freedom\n#&gt; Multiple R-squared:  0.4941, Adjusted R-squared:  0.4905 \n#&gt; F-statistic: 140.1 on 51 and 7316 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nI believe I will use the second one with as.factor as it provides estimates for each hospital and also each month rather than all hospitals in all months in the first one."
  },
  {
    "objectID": "content/01_journal/09_iv.html#load-data-and-libraries",
    "href": "content/01_journal/09_iv.html#load-data-and-libraries",
    "title": "Instrumental Variables",
    "section": "",
    "text": "library(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.3     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dagitty)\nlibrary(ggdag)\n\n#&gt; \n#&gt; Attaching package: 'ggdag'\n#&gt; \n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n\nlibrary(estimatr)\n\napp &lt;- readRDS(\"~/Documents/TUHH/Casual Data Science for Business Analytics/Causal_Data_Science_Data/rand_enc.rds\")\n\napp"
  },
  {
    "objectID": "content/01_journal/09_iv.html#dag-model",
    "href": "content/01_journal/09_iv.html#dag-model",
    "title": "Instrumental Variables",
    "section": "",
    "text": "app_model &lt;- dagify(\n  F ~ P,\n  F ~ U,\n  S ~ F,\n  S ~ U,\n  coords = list(x = c(P = 1,F = 3,U = 5.5,S = 8),\n                y = c(P = 0,F = 0,U = 1.5,S = 0)),\n  labels = list(P = \"PopUp / Encouragement\",\n                F = \"New Feature\",\n                U = \"Unobserved Confounder\",\n                S = \"Screen Time\")\n)\n\n#Plot DAG model\nggdag(app_model) +\n  theme_dag()+\n  geom_dag_node(color = \"red\")+\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"black\") +\n  geom_dag_label_repel(aes(label = label))"
  },
  {
    "objectID": "content/01_journal/09_iv.html#naive-biased-estimate",
    "href": "content/01_journal/09_iv.html#naive-biased-estimate",
    "title": "Instrumental Variables",
    "section": "",
    "text": "#Naive Biased Estimate\n\nlm_biased &lt;- lm(time_spent ~ used_ftr, data = app)\nsummary(lm_biased)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = time_spent ~ used_ftr, data = app)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -20.4950  -3.5393   0.0158   3.5961  20.5051 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 18.86993    0.06955   271.3   &lt;2e-16 ***\n#&gt; used_ftr    10.82269    0.10888    99.4   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 5.351 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.497,  Adjusted R-squared:  0.497 \n#&gt; F-statistic:  9881 on 1 and 9998 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/09_iv.html#testing-assumptions",
    "href": "content/01_journal/09_iv.html#testing-assumptions",
    "title": "Instrumental Variables",
    "section": "",
    "text": "app_assume &lt;- app %&gt;%\n  filter(used_ftr == 1)\n\ncor(app_assume$rand_enc,app_assume$time_spent)\n\n#&gt; [1] -0.02744462\n\n\n\n\nThe value of the correlation between the IV variable and the outcome given the treatment is very small at -0.0274 which is not exactly zero but very small hence it is appropriate to use it specially that we most probably have a lot of unobserved confounders that can’t be all compensated/accounted for with a single IV variable."
  },
  {
    "objectID": "content/01_journal/09_iv.html#computing-iv-estimate",
    "href": "content/01_journal/09_iv.html#computing-iv-estimate",
    "title": "Instrumental Variables",
    "section": "",
    "text": "model_iv &lt;- iv_robust(time_spent ~ used_ftr | rand_enc, data = app)\nsummary(model_iv)\n\n#&gt; \n#&gt; Call:\n#&gt; iv_robust(formula = time_spent ~ used_ftr | rand_enc, data = app)\n#&gt; \n#&gt; Standard error type:  HC2 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value  Pr(&gt;|t|) CI Lower CI Upper   DF\n#&gt; (Intercept)   19.312     0.2248   85.89 0.000e+00   18.872    19.75 9998\n#&gt; used_ftr       9.738     0.5353   18.19 8.716e-73    8.689    10.79 9998\n#&gt; \n#&gt; Multiple R-squared:  0.4921 ,    Adjusted R-squared:  0.492 \n#&gt; F-statistic:   331 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nThe naive estimate appears to be biased yes with a bit of upward bias as the naive estimate is higher than the IV estimate."
  },
  {
    "objectID": "content/01_journal/10_rdd.html#st-campaign",
    "href": "content/01_journal/10_rdd.html#st-campaign",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "library(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.3     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rddensity)\n\ndf &lt;- readRDS(\"~/Documents/TUHH/Casual Data Science for Business Analytics/Causal_Data_Science_Data/coupon.rds\")\ndf\n\n\n\n  \n\n\n# Define cut-off\nc0 &lt;- 60\n\n# Define all 3 bandwidths\nbw &lt;- c0 + c(-5, 5)\nbw_1 &lt;- c0 + c(-2.5,2,5) # Half\nbw_2 &lt;- c0 + c(-10,10)   # Double\n\n\n\n\n\n# Subsets below and above threshold in specified bandwidth\ndf_bw_below &lt;- df %&gt;% filter(days_since_last &gt;= bw[1] & days_since_last &lt; c0)\ndf_bw_above &lt;- df %&gt;% filter(days_since_last &gt;= c0 & days_since_last &lt;= bw[2])\n\ndf_bw &lt;- bind_rows(df_bw_above, df_bw_below)\n\nlm_bw &lt;- lm(purchase_after ~ days_since_last_centered + coupon, df_bw)\nsummary(lm_bw)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_after ~ days_since_last_centered + coupon, \n#&gt;     data = df_bw)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -11.4966  -2.1312  -0.0949   2.0185  10.4159 \n#&gt; \n#&gt; Coefficients:\n#&gt;                          Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)               11.4242     0.3965  28.813  &lt; 2e-16 ***\n#&gt; days_since_last_centered   0.3835     0.1259   3.046  0.00251 ** \n#&gt; couponTRUE                 7.9334     0.7087  11.194  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.186 on 320 degrees of freedom\n#&gt; Multiple R-squared:  0.7074, Adjusted R-squared:  0.7055 \n#&gt; F-statistic: 386.8 on 2 and 320 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n# Case of Half the bandwidth\ndf_bw_below_1 &lt;- df %&gt;% filter(days_since_last &gt;= bw_1[1] & days_since_last &lt; c0)\ndf_bw_above_1 &lt;- df %&gt;% filter(days_since_last &gt;= c0 & days_since_last &lt;= bw_1[2])\n\ndf_bw_1 &lt;- bind_rows(df_bw_above_1, df_bw_below_1)\n\nlm_bw_1 &lt;- lm(purchase_after ~ days_since_last_centered + coupon, df_bw_1)\nsummary(lm_bw_1)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_after ~ days_since_last_centered + coupon, \n#&gt;     data = df_bw_1)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -10.9756  -2.1067  -0.0376   2.0936   8.0708 \n#&gt; \n#&gt; Coefficients:\n#&gt;                          Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)               11.3543     0.6438  17.636  &lt; 2e-16 ***\n#&gt; days_since_last_centered   0.4740     0.3774   1.256    0.211    \n#&gt; couponTRUE                 7.4823     1.0771   6.947 9.38e-11 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.352 on 157 degrees of freedom\n#&gt; Multiple R-squared:  0.6255, Adjusted R-squared:  0.6208 \n#&gt; F-statistic: 131.1 on 2 and 157 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n# Case of Double the bandwidth\ndf_bw_below_2 &lt;- df %&gt;% filter(days_since_last &gt;= bw_2[1] & days_since_last &lt; c0)\ndf_bw_above_2 &lt;- df %&gt;% filter(days_since_last &gt;= c0 & days_since_last &lt;= bw_2[2])\n\ndf_bw_2 &lt;- bind_rows(df_bw_above_2, df_bw_below_2)\n\nlm_bw_2 &lt;- lm(purchase_after ~ days_since_last_centered + coupon, df_bw_2)\nsummary(lm_bw_2)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_after ~ days_since_last_centered + coupon, \n#&gt;     data = df_bw_2)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -12.2718  -2.0858  -0.0003   2.0275  10.6749 \n#&gt; \n#&gt; Coefficients:\n#&gt;                          Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)              10.61700    0.27386  38.767   &lt;2e-16 ***\n#&gt; days_since_last_centered  0.01413    0.04255   0.332     0.74    \n#&gt; couponTRUE                9.51584    0.48628  19.569   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.115 on 626 degrees of freedom\n#&gt; Multiple R-squared:  0.7052, Adjusted R-squared:  0.7042 \n#&gt; F-statistic: 748.6 on 2 and 626 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nThe bandwidth indeed affect the estimate, well at first it didn’t appear like it when we used half the bandwidth only but in the second case of double the original bandwidth we saw a significant change and this tells us that depending on our bandwidth and number of cases within that bandwidth we may get varying results so its important to choose the right bandwidth according to theoretical knowledge of the project which would help us yield better results. I beleive it is one of the parameters that would vary differently from cases to case and must be tuned carefully."
  },
  {
    "objectID": "content/01_journal/10_rdd.html#load-data",
    "href": "content/01_journal/10_rdd.html#load-data",
    "title": "Regression Discontinuity",
    "section": "Load Data",
    "text": "Load Data\n\nlibrary(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.3     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rddensity)\n\ndf &lt;- readRDS(\"~/Documents/TUHH/Casual Data Science for Business Analytics/Causal_Data_Science_Data/coupon.rds\")\ndf\n\n\n\n  \n\n\n# Define cut-off\nc0 &lt;- 60\n\n# Define all 3 bandwidths\nbw &lt;- c0 + c(-5, 5)\nbw_1 &lt;- c0 + c(-2.5,2,5) # Half\nbw_2 &lt;- c0 + c(-10,10)   # Double"
  },
  {
    "objectID": "content/01_journal/10_rdd.html#normal-bandwidth",
    "href": "content/01_journal/10_rdd.html#normal-bandwidth",
    "title": "Regression Discontinuity",
    "section": "Normal Bandwidth",
    "text": "Normal Bandwidth\n\n# Subsets below and above threshold in specified bandwidth\ndf_bw_below &lt;- df %&gt;% filter(days_since_last &gt;= bw[1] & days_since_last &lt; c0)\ndf_bw_above &lt;- df %&gt;% filter(days_since_last &gt;= c0 & days_since_last &lt;= bw[2])\n\ndf_bw &lt;- bind_rows(df_bw_above, df_bw_below)\n\nlm_bw &lt;- lm(purchase_after ~ days_since_last_centered + coupon, df_bw)\nsummary(lm_bw)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_after ~ days_since_last_centered + coupon, \n#&gt;     data = df_bw)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -11.4966  -2.1312  -0.0949   2.0185  10.4159 \n#&gt; \n#&gt; Coefficients:\n#&gt;                          Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)               11.4242     0.3965  28.813  &lt; 2e-16 ***\n#&gt; days_since_last_centered   0.3835     0.1259   3.046  0.00251 ** \n#&gt; couponTRUE                 7.9334     0.7087  11.194  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.186 on 320 degrees of freedom\n#&gt; Multiple R-squared:  0.7074, Adjusted R-squared:  0.7055 \n#&gt; F-statistic: 386.8 on 2 and 320 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/10_rdd.html#half-of-the-normal-bandwidth",
    "href": "content/01_journal/10_rdd.html#half-of-the-normal-bandwidth",
    "title": "Regression Discontinuity",
    "section": "Half of the normal Bandwidth",
    "text": "Half of the normal Bandwidth\n\n# Case of Half the bandwidth\ndf_bw_below_1 &lt;- df %&gt;% filter(days_since_last &gt;= bw_1[1] & days_since_last &lt; c0)\ndf_bw_above_1 &lt;- df %&gt;% filter(days_since_last &gt;= c0 & days_since_last &lt;= bw_1[2])\n\ndf_bw_1 &lt;- bind_rows(df_bw_above_1, df_bw_below_1)\n\nlm_bw_1 &lt;- lm(purchase_after ~ days_since_last_centered + coupon, df_bw_1)\nsummary(lm_bw_1)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_after ~ days_since_last_centered + coupon, \n#&gt;     data = df_bw_1)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -10.9756  -2.1067  -0.0376   2.0936   8.0708 \n#&gt; \n#&gt; Coefficients:\n#&gt;                          Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)               11.3543     0.6438  17.636  &lt; 2e-16 ***\n#&gt; days_since_last_centered   0.4740     0.3774   1.256    0.211    \n#&gt; couponTRUE                 7.4823     1.0771   6.947 9.38e-11 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.352 on 157 degrees of freedom\n#&gt; Multiple R-squared:  0.6255, Adjusted R-squared:  0.6208 \n#&gt; F-statistic: 131.1 on 2 and 157 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/10_rdd.html#double-the-normal-bandwidth",
    "href": "content/01_journal/10_rdd.html#double-the-normal-bandwidth",
    "title": "Regression Discontinuity",
    "section": "Double the normal Bandwidth",
    "text": "Double the normal Bandwidth\n\n# Case of Double the bandwidth\ndf_bw_below_2 &lt;- df %&gt;% filter(days_since_last &gt;= bw_2[1] & days_since_last &lt; c0)\ndf_bw_above_2 &lt;- df %&gt;% filter(days_since_last &gt;= c0 & days_since_last &lt;= bw_2[2])\n\ndf_bw_2 &lt;- bind_rows(df_bw_above_2, df_bw_below_2)\n\nlm_bw_2 &lt;- lm(purchase_after ~ days_since_last_centered + coupon, df_bw_2)\nsummary(lm_bw_2)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_after ~ days_since_last_centered + coupon, \n#&gt;     data = df_bw_2)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -12.2718  -2.0858  -0.0003   2.0275  10.6749 \n#&gt; \n#&gt; Coefficients:\n#&gt;                          Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)              10.61700    0.27386  38.767   &lt;2e-16 ***\n#&gt; days_since_last_centered  0.01413    0.04255   0.332     0.74    \n#&gt; couponTRUE                9.51584    0.48628  19.569   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.115 on 626 degrees of freedom\n#&gt; Multiple R-squared:  0.7052, Adjusted R-squared:  0.7042 \n#&gt; F-statistic: 748.6 on 2 and 626 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/10_rdd.html#conclusion",
    "href": "content/01_journal/10_rdd.html#conclusion",
    "title": "Regression Discontinuity",
    "section": "Conclusion",
    "text": "Conclusion\nThe bandwidth indeed affect the estimate, well at first it didn’t appear like it when we used half the bandwidth only but in the second case of double the original bandwidth we saw a significant change and this tells us that depending on our bandwidth and number of cases within that bandwidth we may get varying results so its important to choose the right bandwidth according to theoretical knowledge of the project which would help us yield better results. I believe it is one of the parameters that would vary differently from cases to case and must be tuned carefully."
  },
  {
    "objectID": "content/01_journal/09_iv.html#argument",
    "href": "content/01_journal/09_iv.html#argument",
    "title": "Instrumental Variables",
    "section": "Argument",
    "text": "Argument\nThe value of the correlation between the IV variable and the outcome given the treatment is very small at -0.0274 which is not exactly zero but very small hence it is appropriate to use it specially that we most probably have a lot of unobserved confounders that can’t be all compensated/accounted for with a single IV variable."
  },
  {
    "objectID": "content/01_journal/09_iv.html#naive-vs-iv",
    "href": "content/01_journal/09_iv.html#naive-vs-iv",
    "title": "Instrumental Variables",
    "section": "Naive vs IV",
    "text": "Naive vs IV\nThe naive estimate appears to be biased yes with a bit of upward bias as the naive estimate is higher than the IV estimate."
  },
  {
    "objectID": "content/01_journal/10_rdd.html#load-data-1",
    "href": "content/01_journal/10_rdd.html#load-data-1",
    "title": "Regression Discontinuity",
    "section": "Load Data",
    "text": "Load Data\n\nlibrary(rddensity)\nshipping &lt;- readRDS(\"~/Documents/TUHH/Casual Data Science for Business Analytics/Causal_Data_Science_Data/shipping.rds\")\n\nshipping"
  },
  {
    "objectID": "content/01_journal/10_rdd.html#purchase_amount",
    "href": "content/01_journal/10_rdd.html#purchase_amount",
    "title": "Regression Discontinuity",
    "section": "purchase_amount",
    "text": "purchase_amount\n\ncs = 30\n\nrddd &lt;- rddensity(shipping$purchase_amount, c = cs)\nsummary(rddd)\n\n#&gt; \n#&gt; Manipulation testing using local polynomial density estimation.\n#&gt; \n#&gt; Number of obs =       6666\n#&gt; Model =               unrestricted\n#&gt; Kernel =              triangular\n#&gt; BW method =           estimated\n#&gt; VCE method =          jackknife\n#&gt; \n#&gt; c = 30                Left of c           Right of c          \n#&gt; Number of obs         3088                3578                \n#&gt; Eff. Number of obs    2221                1955                \n#&gt; Order est. (p)        2                   2                   \n#&gt; Order bias (q)        3                   3                   \n#&gt; BW est. (h)           22.909              20.394              \n#&gt; \n#&gt; Method                T                   P &gt; |T|             \n#&gt; Robust                5.9855              0\n\n\n#&gt; Warning in summary.CJMrddensity(rddd): There are repeated observations. Point\n#&gt; estimates and standard errors have been adjusted. Use option massPoints=FALSE\n#&gt; to suppress this feature.\n\n\n#&gt; \n#&gt; P-values of binomial tests (H0: p=0.5).\n#&gt; \n#&gt; Window Length / 2          &lt;c     &gt;=c    P&gt;|T|\n#&gt; 0.261                      20      26    0.4614\n#&gt; 0.522                      41      65    0.0250\n#&gt; 0.783                      62     107    0.0007\n#&gt; 1.043                      81     136    0.0002\n#&gt; 1.304                     100     169    0.0000\n#&gt; 1.565                     114     196    0.0000\n#&gt; 1.826                     132     227    0.0000\n#&gt; 2.087                     156     263    0.0000\n#&gt; 2.348                     173     298    0.0000\n#&gt; 2.609                     191     331    0.0000\n\nrdd_plot &lt;- rdplotdensity(rddd, shipping$purchase_amount, plotN = 100)"
  },
  {
    "objectID": "content/01_journal/10_rdd.html#conclusion-1",
    "href": "content/01_journal/10_rdd.html#conclusion-1",
    "title": "Regression Discontinuity",
    "section": "Conclusion",
    "text": "Conclusion\nWe cannot use purchase_amount as a running variable with a cut-off at 30€ as first of all the p-value is less than any common \\(\\alpha\\) value plus the plot shows no overlapping of the confidence intervals and shows us discontinuity in the running variable resulting that we can’t use RDD obtain valid results"
  }
]